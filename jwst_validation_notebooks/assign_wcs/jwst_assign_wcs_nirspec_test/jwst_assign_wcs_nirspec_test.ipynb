{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Testing Notebook: NIRSpec, spec2, assign_wcs step\n",
    "\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: NIRSpec \n",
    "\n",
    "Tested on CV3 data\n",
    "\n",
    "### Table of Contents\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "<br> [Imports](#imports_ID) <br> [Introduction](#intro_ID) <br> [Testing Data Set](#data_ID) <br> [Run the JWST pipeline and assign_wcs validation tests](#pipeline_ID): [FS Full-Frame test](#FULLFRAME), [FS ALLSLITS test](#ALLSLITS), [MOS test](#MOS), [IFU test](#IFU) <br> [About This Notebook](#about_ID)<br> [Results](#results) <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import shutil\n",
    "data_dir = TemporaryDirectory()\n",
    "\n",
    "os.chdir(data_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose CRDS cache location\n",
    "use_local_crds_cache = False\n",
    "crds_cache_tempdir = False\n",
    "crds_cache_notebook_dir = True\n",
    "crds_cache_home = False\n",
    "crds_cache_custom_dir = False\n",
    "crds_cache_dir_name = \"\"\n",
    "\n",
    "if use_local_crds_cache:\n",
    "    if crds_cache_tempdir:\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.getcwd(), \"crds\")\n",
    "    elif crds_cache_notebook_dir:\n",
    "        try:\n",
    "            os.environ['CRDS_PATH'] = os.path.join(orig_dir, \"crds\")\n",
    "        except Exception as e:\n",
    "            os.environ['CRDS_PATH'] = os.path.join(os.getcwd(), \"crds\")\n",
    "    elif crds_cache_home:\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'], 'crds', 'cache')\n",
    "    elif crds_cache_custom_dir:\n",
    "        os.environ['CRDS_PATH'] = crds_cache_dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports_ID\"></a>\n",
    "# Imports\n",
    "The library imports relevant to this notebook are aready taken care of by importing PTT.\n",
    "\n",
    "* astropy.io for opening fits files\n",
    "* jwst.module.PipelineStep is the pipeline step being tested\n",
    "* matplotlib.pyplot.plt to generate plot\n",
    "\n",
    "NOTE: This notebook assumes that the pipeline version to be tested is already installed and its environment is activated.\n",
    "\n",
    "To be able to run this notebook you need to install nptt. \n",
    "\n",
    "If all goes well you will be able to import PTT.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45177853-942e-4949-9e30-f544d70ef5f4"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import psutil\n",
    "from astropy.io import fits\n",
    "\n",
    "# Only print a DeprecationWarning the first time it shows up, not every time.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"once\", category=DeprecationWarning)\n",
    "    import jwst\n",
    "    from jwst.pipeline.calwebb_detector1 import Detector1Pipeline\n",
    "    from jwst.assign_wcs.assign_wcs_step import AssignWcsStep\n",
    "\n",
    "# The latest version of NPTT is installed in the requirements text file at:\n",
    "# /jwst_validation_notebooks/environment.yml\n",
    "\n",
    "# import NPTT\n",
    "import nirspec_pipe_testing_tool as nptt\n",
    "\n",
    "# To get data from Artifactory\n",
    "from ci_watson.artifactory_helpers import get_bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the version used is the right one\n",
    "\n",
    "pipeline_version = jwst.__version__\n",
    "nptt_version = nptt.__version__\n",
    "\n",
    "print(\"Using jwst pipeline version: \", pipeline_version)\n",
    "print(\"Using NPTT version: \", nptt_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro_ID\"></a>\n",
    "# Test Description\n",
    "\n",
    "We compared Institute's pipeline product of the assign_wcs step with our benchmark files, or with the intermediary products from the ESA pipeline, which is completely independent from the Institute's. The comparison file is referred to as 'truth'. We calculated the relative difference and expected it to be equal to or less than computer precision:  relative_difference = absolute_value( (Truth - ST)/Truth )  <= 1x10^-7. \n",
    "\n",
    "For the test to be considered PASSED, every single slit (for FS data), slitlet (for MOS data) or slice (for IFU data) in the input file has to pass. If there is any failure, the whole test will be considered as FAILED. \n",
    "\n",
    "The code for this test for Fixed Slits (FS) can be obtained from: https://github.com/spacetelescope/nirspec_pipe_testing_tool/blob/master/nirspec_pipe_testing_tool/calwebb_spec2_pytests/auxiliary_code/compare_wcs_fs.py. For Multi Object Spectroscopy (MOS), the code is in the same repository but is named ```compare_wcs_mos.py```, and for Integral Field Unit (IFU) data, the test is named ```compare_wcs_ifu.py```.\n",
    "The input file is defined in the variable ```input_file``` (see section [Testing Data Set and Variable Setup](#data_ID)).\n",
    "\n",
    "Step description: https://jwst-pipeline.readthedocs.io/en/latest/jwst/assign_wcs/main.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst/assign_wcs\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "If the test **PASSED** this means that all slits, slitlets, or slices individually passed the test. However, if ony one individual slit (for FS data), slitlet (for MOS data) or slice (for IFU data) test failed, the whole test will be reported as **FAILED**.### Calibration WG Requested Algorithm: \n",
    "\n",
    "\n",
    "### Calibration WG Requested Algorithm: \n",
    "\n",
    "A short description and link to the page: https://outerspace.stsci.edu/display/JWSTCC/Vanilla+Spectral+GWCS+InformationÂ \n",
    "\n",
    "\n",
    "### Defining Term\n",
    "Acronymns used un this notebook:\n",
    "\n",
    "pipeline: calibration pipeline\n",
    "\n",
    "spec2: spectroscopic calibration pipeline level 2b\n",
    "\n",
    "PTT: NIRSpec pipeline testing tool (https://github.com/spacetelescope/nirspec_pipe_testing_tool)\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline_ID\"></a>\n",
    "# Run the JWST pipeline and assign_wcs validation tests\n",
    "\n",
    "The pipeline can be run from the command line in two variants: full or per step.\n",
    "\n",
    "Tu run the spec2 pipeline in full use the command: \n",
    "\n",
    "$ strun jwst.pipeline.Spec2Pipeline jwtest_rate.fits\n",
    "\n",
    "Tu only run the assign_wcs step, use the command:\n",
    "\n",
    "$ strun jwst.assign_wcs.AssignWcsStep jwtest_rate.fits\n",
    "\n",
    "NIRSpec TA data will be run through the cal_detector1 and the imaging2 pipelines. The imaging pipeline can be run with the following fommand:\n",
    "\n",
    "$ strun jwst.pipeline.Image2Pipeline jwtest_rate.fits\n",
    "\n",
    "These options are also callable from a script with the testing environment active. The Python call for running the pipeline in full or by step are:\n",
    "\n",
    "$\\gt$ from jwst.pipeline.calwebb_spec2 import Spec2Pipeline\n",
    "\n",
    "$\\gt$ Spec2Pipeline.call(jwtest_rate.fits)\n",
    " \n",
    "or\n",
    " \n",
    "$\\gt$ from jwst.assign_wcs.assign_wcs_step import AssignWcsStep\n",
    " \n",
    "$\\gt$ AssignWcsStep.call(jwtest_rate.fits)\n",
    "\n",
    "For the imaging pipeline the call would be as follows:\n",
    "\n",
    "$\\gt$ from jwst.pipeline.calwebb_image2 import Image2Pipeline\n",
    "\n",
    "$\\gt$ Image2Pipeline.call(jwtest_rate.fits)\n",
    "\n",
    "NPTT can run the spec2 pipeline either in full or per step, as well as the imaging pipeline in full. In this notebook we will use NPTT to run the pipeline and the validation tests. To run NPTT, follow the directions in the corresponding repo page.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_ID\"></a>\n",
    "# Testing Data Set\n",
    "\n",
    "All testing data is from the CV3 campaign. We chose these files because this is our most complete data set, i.e. all modes and filter-grating combinations.\n",
    "\n",
    "Data used was for testing:\n",
    "- FS_PRISM_CLEAR\n",
    "- FS_FULLFRAME_G395H_F290LP\n",
    "- FS_ALLSLITS_G140H_F100LP \n",
    "- MOS_G140M_LINE1 \n",
    "- MOS_PRISM_CLEAR\n",
    "- IFU_G395H_F290LP \n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = {'fs_prism_clear':{\n",
    "                                  'uncal_file_nrs1': 'fs_prism_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'fs_prism_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'fs_prism_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': 'fs_prism_nrs2_assign_wcs_truth.fits',\n",
    "                                  'msa_shutter_config': None },\n",
    "                \n",
    "                'fs_fullframe_g395h_f290lp':{\n",
    "                                  'uncal_file_nrs1': 'fs_fullframe_g395h_f290lp_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'fs_fullframe_g395h_f290lp_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'fs_fullframe_g395h_f290lp_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': 'fs_fullframe_g395h_f290lp_nrs2_assign_wcs_truth.fits',  \n",
    "                                  'msa_shutter_config': None },\n",
    "                \n",
    "                'fs_allslits_g140h_f100lp':{\n",
    "                                  'uncal_file_nrs1': 'fs_allslits_g140h_f100lp_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'fs_allslits_g140h_f100lp_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'fs_allslits_g140h_f100lp_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': 'fs_allslits_g140h_f100lp_nrs2_assign_wcs_truth.fits',\n",
    "                                  'msa_shutter_config': None },\n",
    "                \n",
    "                # Commented out because the pipeline is failing with this file\n",
    "                #'bots_g235h_f170lp':{\n",
    "                #                  'uncal_file_nrs1': 'bots_g235h_f170lp_nrs1_uncal.fits',\n",
    "                #                  'uncal_file_nrs2': 'bots_g235h_f170lp_nrs2_uncal.fits',\n",
    "                #                  'truth_file_nrs1': 'bots_g235h_f170lp_nrs1_assign_wcs_truth.fits',\n",
    "                #                  'truth_file_nrs2': 'bots_g235h_f170lp_nrs2_assign_wcs_truth.fits',\n",
    "                #                  'msa_shutter_config': None },\n",
    "                \n",
    "                'mos_prism_clear':{\n",
    "                                  'uncal_file_nrs1': 'mos_prism_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'mos_prism_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'mos_prism_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': None,\n",
    "                                  'msa_shutter_config': 'V0030006000104_msa.fits' },\n",
    "                \n",
    "                'mos_g140m_f100lp':{\n",
    "                                  'uncal_file_nrs1': 'mos_g140m_line1_NRS1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'mos_g140m_line1_NRS2_uncal.fits',  \n",
    "                                  'truth_file_nrs1': 'mos_g140m_line1_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': 'mos_g140m_line1_nrs2_assign_wcs_truth.fits',\n",
    "                                  'msa_shutter_config': 'V8460001000101_msa.fits' },\n",
    "                \n",
    "                'ifu_prism_clear':{\n",
    "                                  'uncal_file_nrs1': 'ifu_prism_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'ifu_prism_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'ifu_prism_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': None,\n",
    "                                  'msa_shutter_config': None },\n",
    "               \n",
    "                'ifu_g395h_f290lp':{\n",
    "                                  'uncal_file_nrs1': 'ifu_g395h_f290lp_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'ifu_g395h_f290lp_nrs2_uncal.fits',\n",
    "                                  'truth_file_nrs1': 'ifu_g395h_f290lp_nrs1_assign_wcs_truth.fits',\n",
    "                                  'truth_file_nrs2': 'ifu_g395h_f290lp_nrs2_assign_wcs_truth.fits',\n",
    "                                  'msa_shutter_config': None }\n",
    "\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to pull data from Artifactory\n",
    "def get_artifactory_file(data_set_dict, detector):\n",
    "    \"\"\"This function creates a list with all the files needed per detector to run the test.\n",
    "    Args:\n",
    "        data_set_dict: dictionary, contains inputs for a specific mode and configuration\n",
    "        detector: string, either nrs1 or nrs2\n",
    "    Returns:\n",
    "        data: list, contains all files needed to run test\n",
    "    \"\"\"\n",
    "    files2obtain = ['uncal_file_nrs1', 'truth_file_nrs1', 'msa_shutter_config']\n",
    "    data = []\n",
    "    for file in files2obtain:\n",
    "        data_file = None\n",
    "        try: \n",
    "            if '_nrs' in file and '2' in detector:\n",
    "                file = file.replace('_nrs1', '_nrs2')\n",
    "\n",
    "            data_file = get_bigdata('jwst_validation_notebooks',\n",
    "                                         'validation_data',\n",
    "                                         'nirspec_data', \n",
    "                                         data_set_dict[file])\n",
    "        except TypeError:\n",
    "            data.append(None)\n",
    "            continue\n",
    "\n",
    "        data.append(data_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common NPTT switches for this test and run the test for both detectors for each data set\n",
    "\n",
    "# define benchmark (or 'truth') file\n",
    "compare_assign_wcs_and_extract_2d_with_esa = False\n",
    "\n",
    "# accepted threshold difference with respect to benchmark files\n",
    "threshold_diff = 1e-7\n",
    "\n",
    "# other variables\n",
    "esa_files_path, raw_data_root_file = None, None\n",
    "save_figs = False\n",
    "show_figs = True\n",
    "\n",
    "# Get the data\n",
    "detectors = ['nrs1', 'nrs2']\n",
    "results_dict = {}\n",
    "for mode_config, data_set_dict in testing_data.items():\n",
    "    for det in detectors:\n",
    "        print('Testing files for detector: ', det)\n",
    "        data = get_artifactory_file(data_set_dict, det)\n",
    "        uncal_file, truth_file, msa_shutter_config = data\n",
    "        uncal_basename = os.path.basename(uncal_file)\n",
    "        print('Working with uncal_file: ', uncal_basename)\n",
    "        \n",
    "        # Make sure that there is an assign_wcs truth product to compare to, else skip this data set\n",
    "        if truth_file is None:\n",
    "            print('No truth file to compare to for this detector, skipping this data set.  \\n')\n",
    "            skip_file = True\n",
    "\n",
    "        # Run the stage 1 pipeline \n",
    "        rate_object = Detector1Pipeline.call(uncal_file)\n",
    "        \n",
    "        # Make sure the MSA shutter configuration file is set up correctly\n",
    "        if msa_shutter_config is not None:\n",
    "            msa_metadata = rate_object.meta.instrument.msa_metadata_file\n",
    "            if msa_metadata is None or msa_metadata == 'N/A':\n",
    "                rate_object.meta.instrument.msa_metadata_file = msa_shutter_config\n",
    "\n",
    "        # Run the stage 2 pipeline steps\n",
    "        try:\n",
    "            assign_wcs_object = AssignWcsStep.call(rate_object)\n",
    "            skip_file = False\n",
    "        except:\n",
    "            print(\"No open slits fall on detector \", det)\n",
    "            print(\"Skipping test for this file. \\n\")\n",
    "            skip_file = True\n",
    "            \n",
    "        if not skip_file:  \n",
    "            # Run the validation test\n",
    "            %matplotlib inline\n",
    "            if 'fs' in uncal_file.lower():\n",
    "                print('Running test for FS...')\n",
    "                result, _ = nptt.calwebb_spec2_pytests.auxiliary_code.compare_wcs_fs.compare_wcs(\n",
    "                                                                                assign_wcs_object, \n",
    "                                                                                truth_file=truth_file, \n",
    "                                                                                esa_files_path=esa_files_path, \n",
    "                                                                                show_figs=show_figs,\n",
    "                                                                                save_figs=save_figs, \n",
    "                                                                                threshold_diff=threshold_diff, \n",
    "                                                                                raw_data_root_file=raw_data_root_file,\n",
    "                                                                                output_directory=None)     \n",
    "            if 'mos' in uncal_file.lower():\n",
    "                print('Running test for MOS...')\n",
    "                result, _ = nptt.calwebb_spec2_pytests.auxiliary_code.compare_wcs_mos.compare_wcs(\n",
    "                                                                                assign_wcs_object, \n",
    "                                                                                msa_conf_name=msa_shutter_config, \n",
    "                                                                                truth_file=truth_file,\n",
    "                                                                                esa_files_path=esa_files_path, \n",
    "                                                                                show_figs=show_figs, \n",
    "                                                                                save_figs=save_figs, \n",
    "                                                                                threshold_diff=threshold_diff,\n",
    "                                                                                mode_used='mos', \n",
    "                                                                                raw_data_root_file=raw_data_root_file)\n",
    "            if 'ifu' in uncal_file.lower():\n",
    "                print('Running test for IFU...')\n",
    "                result, _ = nptt.calwebb_spec2_pytests.auxiliary_code.compare_wcs_ifu.compare_wcs(\n",
    "                                                                                assign_wcs_object, \n",
    "                                                                                truth_file=truth_file, \n",
    "                                                                                esa_files_path=esa_files_path, \n",
    "                                                                                show_figs=show_figs,\n",
    "                                                                                save_figs=save_figs, \n",
    "                                                                                threshold_diff=threshold_diff, \n",
    "                                                                                raw_data_root_file=raw_data_root_file)\n",
    "\n",
    "\n",
    "        else:\n",
    "            result = 'skipped'\n",
    "\n",
    "        # Did the test passed \n",
    "        print(\"Did assign_wcs validation test passed? \", result, \"\\n\\n\")\n",
    "        rd = {uncal_basename: result}\n",
    "        results_dict.update(rd)\n",
    "        \n",
    "        \"\"\" Commenting this block to avoid an OSError while closing the files\n",
    "        # close all open files\n",
    "        psutil.Process().open_files()\n",
    "        closing_files = []\n",
    "        for fd in psutil.Process().open_files():\n",
    "            if data_dir.name in fd.path:\n",
    "                closing_files.append(fd)\n",
    "        for fd in closing_files:\n",
    "            try:\n",
    "                print('Closing file: ', fd)\n",
    "                open(fd.fd).close()\n",
    "            except:\n",
    "                print('File already closed: ', fd)\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly see if the test passed \n",
    "\n",
    "print('These are the final results of the tests: ')\n",
    "for key, val in results_dict.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about_ID\"></a>\n",
    "## About this Notebook\n",
    "**Author:** Maria A. Pena-Guerrero, Staff Scientist II - Systems Science Support, NIRSpec\n",
    "<br>**Updated On:** Mar/29/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)\n",
    "<img style=\"float: right;\" src=\"./stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"stsci_pri_combo_mark_horizonal_white_bkgd\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
