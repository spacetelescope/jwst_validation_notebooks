{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Notebook: calwebb_image3, NIRCam imaging\n",
    "\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: e.g., NIRCam \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "<div style=\"text-align: left\"> \n",
    "    \n",
    "<br> [Introduction\\*](#intro)\n",
    "<br> [JWST CalWG Algorithm\\*](#algorithm)\n",
    "<br> [Defining Terms](#terms)\n",
    "<br> [Test Description\\*](#description)\n",
    "<br> [Data Description\\*](#data_descr)\n",
    "<br> [Imports\\*](#imports)\n",
    "<br> [Convenience Functions](#convenience_functions)\n",
    "<br> [Loading the Data\\*](#data_load)\n",
    "<br> [calwebb_image3 - Ensemble calibrations](#image3) \n",
    "<br> [Run the entire pipeline](#image3_at_once)\n",
    "<br> [Run the individual pipeline steps](#image3_step_by_step)\n",
    "   <br> [The `WCS Refinement` step](#tweakreg)\n",
    "   <br> [The `Sky Matching` step](#skymatch)\n",
    "   <br> [The `Outlier Detection` step](#outlier_detection)\n",
    "   <br> [The `Resample` step](#resample)\n",
    "   <br> [The `Source Catalog` step](#source_catalog)\n",
    "<br> [About This Notebook\\*](#about)\n",
    "<br>    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "\n",
    "This notebook covers part 3 of the imaging mode data calibration module. In this notebook we'll check Stage 3 of the JWST calibration pipeline for imaging data, also known as *calwebb\\_image3*. \n",
    "\n",
    "The [Stage 3 pipeline](https://jwst-pipeline.readthedocs.io/en/stable/jwst/pipeline/calwebb_image3.html) takes one or more calibrated slope images (`*_cal.fits` files) and combines them into a final mosaic image. It then creates a source catalog from this mosaic. Several steps are performed in order to prepare the data for the mosaic creation. These steps largely mirror what is done by [DrizzlePac](https://www.stsci.edu/scientific-community/software/drizzlepac.html) software when working with HST data. \n",
    "\n",
    "First, using common sources found across the input images, the WCS of each image is refined. Background levels are then matched across the inputs. Spurious sources (e.g. cosmic rays that were not flagged in the `jump` step during Stage 1 processing) are removed by comparing each individual input image to a median image. The indivudal images are combined into a single mosaic image. A source catalog is created based on the mosaic image. And finally, the individual exposures are updated using the information from the preceding steps. New versions of the individual calibrated slope images (`*_cal.fits` files) are produced that contain matched backgrounds, flagged spurious sources, and improved WCS objects. Also, updated [resampled](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/main.html) images (`_i2d.fits` files) are created which all contain the final undistorted sky projection that is present in the mosaic image.\n",
    "\n",
    "There are three final outputs. The first is updated copies of the input files. These updated files contain a consistent WCS, such that they overlap correctly. The second output is a final mosaic image created by drizzling the input images onto a distortion-free grid. And the final output is a source catalog wth basic photometry, created from the final mosaic image.\n",
    "\n",
    "To check how the steps of the pipeline change the input data, we will use several NIRCam simulated data files and run them through the pipeline, examining the results at several places along the way.\n",
    "\n",
    "All JWST imaging mode data, regardless of instrument, are processed through the *calwebb\\_image3* pipeline. The steps and the order in which they are performed is the same for all data.\n",
    "\n",
    "Pipeline description: https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image3.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algorithm\"></a>\n",
    "# JWST CalWG Algorithm\n",
    "\n",
    "Multiple exposures from the direct imaging modes are combined into a combined image (e.g., mosaic).\n",
    "The current status of the algorithms for this pipeline stage is summarized in the link below. Links are provided to individual pages where the details of the algorithms are given along with notes on why those algorithms were picked.\n",
    "\n",
    "The algorithms for each step in each pipeline stage are split into baseline\" and \"enhanced\" versions (formerly known as \"vanilla\" and \"optimal\", respectively).  See Baseline and Enhanced Algorithms for more details.\n",
    "Input/Outputs of this stage refer to the main data products for the pipeline process. The full list of archive products for this (and all stages of the pipeline) is tabulated in Archive Products.\n",
    "\n",
    "[JWST CalWG algorithms for calwebb_image3](https://outerspace.stsci.edu/display/JWSTCC/CALWEBB_IMAGE3)\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"terms\"></a>\n",
    "# Defining Terms\n",
    "\n",
    "\n",
    "**JWST**: James Webb Space Telescope\n",
    "\n",
    "**NIR**: Near Infrared\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"description\"></a>\n",
    "# Test Description\n",
    "\n",
    "This test is performed by running simulated data through the full pipeline and performing a visual inspection of the outputs. Next, the notebook does quick checks after each step in the calwebb_image3 pipeline, based on the algorithms defined. \n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_descr\"></a>\n",
    "# Data Description\n",
    "\n",
    "For this module, we will use an association of calibrated NIRCam simulated imaging exposures generated with Mirage.\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tempdir\"></a>\n",
    "# Set up Temporary Directory\n",
    "The following cell sets up a temporary directory (using python's `tempfile.TemporaryDirectory()`), and changes the script's active directory into that directory (using python's `os.chdir()`). This is so that, when the notebook is run through, it will download files to (and create output files in) the temporary directory rather than in the notebook's directory. This makes cleanup significantly easier (since all output files are deleted when the notebook is shut down), and also means that different notebooks in the same directory won't interfere with each other when run by the automated webpage generation process.\n",
    "\n",
    "If you want the notebook to generate output in the notebook's directory, simply don't run this cell.\n",
    "\n",
    "If you have a file (or files) that are kept in the notebook's directory, and that the notebook needs to use while running, you can copy that file into the directory (the code to do so is present below, but commented out).\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****\n",
    "#\n",
    "# Set this variable to False to not use the temporary directory\n",
    "#\n",
    "#****\n",
    "use_tempdir = True\n",
    "\n",
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if use_tempdir:\n",
    "    data_dir = TemporaryDirectory()\n",
    "\n",
    "    # If you have files that are in the notebook's directory, but that the notebook will need to use while\n",
    "    # running, copy them into the temporary directory here.\n",
    "    #\n",
    "    # files = ['name_of_file']\n",
    "    # for file_name in files:\n",
    "    #     shutil.copy(file_name, os.path.join(data_dir.name, file_name))\n",
    "\n",
    "    # Save original directory\n",
    "    orig_dir = os.getcwd()\n",
    "\n",
    "    # Move to new directory\n",
    "    os.chdir(data_dir.name)\n",
    "\n",
    "# For info, print out where the script is running\n",
    "print(\"Running in {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'CRDS_CACHE_TYPE' in os.environ:\n",
    "    if os.environ['CRDS_CACHE_TYPE'] == 'local':\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'], 'crds', 'cache')\n",
    "    elif os.path.isdir(os.environ['CRDS_CACHE_TYPE']):\n",
    "        os.environ['CRDS_PATH'] = os.environ['CRDS_CACHE_TYPE']\n",
    "print('CRDS cache location: {}'.format(os.environ['CRDS_PATH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Imports_ID'></a>\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages necessary for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module with functions to get information about objects:\n",
    "from glob import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Numpy library:\n",
    "import numpy as np\n",
    "\n",
    "# To read association file\n",
    "import json\n",
    "\n",
    "# To download data\n",
    "import requests\n",
    "\n",
    "# To examine parameter reference files\n",
    "import asdf\n",
    "\n",
    "# Astropy tools:\n",
    "from astropy.io import ascii, fits\n",
    "from astropy.utils.data import download_file\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, LogStretch, LinearStretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up matplotlib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Use this version for non-interactive plots (easier scrolling of the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "# Use this version (outside of Jupyter Lab) if you want interactive plots\n",
    "# %matplotlib notebook\n",
    "\n",
    "# These gymnastics are needed to make the sizes of the figures\n",
    "# be the same in both the inline and notebook versions\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches': None}\n",
    "\n",
    "mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import JWST pipeline-related modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire calwebb_image3 pipeline\n",
    "from jwst.pipeline import calwebb_image3\n",
    "\n",
    "# Individual steps that make up calwebb_image3\n",
    "from jwst.tweakreg import TweakRegStep\n",
    "from jwst.skymatch import SkyMatchStep\n",
    "from jwst.outlier_detection import OutlierDetectionStep\n",
    "from jwst.resample import ResampleStep\n",
    "from jwst.source_catalog import SourceCatalogStep\n",
    "from jwst import datamodels\n",
    "from jwst.associations import asn_from_list\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which version of the pipeline we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwst\n",
    "print(jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='convenience_functions'></a>\n",
    "# Define convenience functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions that we will use repeatedly throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files created in this notebook will be saved\n",
    "# in a subdirectory of the base directory called `Stage3`\n",
    "output_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_pix_types(dq_value):\n",
    "    \"\"\"Given an integer representation of a series of bad pixel flags,\n",
    "    identify which types of bad pixels the flags indicate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dq_value : uint16\n",
    "        Value associated with a set of bad pixel flags\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bad_nums : list\n",
    "        List of integers representing the bad pixel types\n",
    "        \n",
    "    bad_types : list\n",
    "        List of bad pixel type names corresponding to bad_nums\n",
    "    \"\"\"\n",
    "    # Change integer into a byte array\n",
    "    bitarr = np.binary_repr(dq_value)\n",
    "    \n",
    "    # Find the bad pixel type associated with each bit where\n",
    "    # the flag is set\n",
    "    bad_nums = []\n",
    "    bad_types = []\n",
    "    for i, elem in enumerate(bitarr[::-1]):\n",
    "        if elem == str(1):\n",
    "            badval = 2**i\n",
    "            bad_nums.append(badval)\n",
    "            key = next(key for key, value in datamodels.dqflags.pixel.items() if value == badval)\n",
    "            bad_types.append(key)\n",
    "    return bad_nums, bad_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_catalog(data_2d, catalog, flux_limit=0, vmin=0, vmax=10,\n",
    "                    title=None, units='MJy/str'):\n",
    "    \"\"\"Function to generate a 2D image of the data, \n",
    "    with sources overlaid.\n",
    "    \n",
    "    data_2d : numpy.ndarray\n",
    "        2D image to be displayed\n",
    "        \n",
    "    catalog : astropy.table.Table\n",
    "        Table of sources\n",
    "    \n",
    "    flux_limit : float\n",
    "        Minimum signal threshold to overplot sources from catalog.\n",
    "        Sources below this limit will not be shown on the image.\n",
    "        \n",
    "    vmin : float\n",
    "        Minimum signal value to use for scaling\n",
    "        \n",
    "    vmax : float\n",
    "        Maximum signal value to use for scaling\n",
    "        \n",
    "    title : str\n",
    "        String to use for the plot title\n",
    "                \n",
    "    units : str\n",
    "        Units of the data. Used for the annotation in the\n",
    "        color bar\n",
    "    \"\"\"\n",
    "    norm = ImageNormalize(data_2d, interval=ManualInterval(vmin=vmin, vmax=vmax),\n",
    "                              stretch=LogStretch())\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(data_2d, origin='lower', norm=norm)\n",
    "    \n",
    "    for row in catalog:\n",
    "        if row['aper_total_flux'].value > flux_limit:\n",
    "            plt.plot(row['xcentroid'], row['ycentroid'], marker='o',\n",
    "                     markersize='3', color='red')\n",
    "\n",
    "    plt.xlabel('Pixel column')\n",
    "    plt.ylabel('Pixel row')\n",
    "    \n",
    "    fig.colorbar(im, label=units)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(left=0.15)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data_2d, vmin, vmax, xpixel=None, ypixel=None, title=None,\n",
    "               scale='log', units='MJy/str'):\n",
    "    \"\"\"Function to generate a 2D, log-scaled image of the data, \n",
    "    with an option to highlight a specific pixel.\n",
    "    \n",
    "    data_2d : numpy.ndarray\n",
    "        2D image to be displayed\n",
    "        \n",
    "    vmin : float\n",
    "        Minimum signal value to use for scaling\n",
    "        \n",
    "    vmax : float\n",
    "        Maximum signal value to use for scaling\n",
    "        \n",
    "    xpixel : int\n",
    "        X-coordinate of pixel to highlight\n",
    "        \n",
    "    ypixel : int\n",
    "        Y-coordinate of pixel to highlight\n",
    "        \n",
    "    title : str\n",
    "        String to use for the plot title\n",
    "        \n",
    "    scale : str\n",
    "        Specify scaling of the image. Can be 'log' or 'linear'\n",
    "        \n",
    "    units : str\n",
    "        Units of the data. Used for the annotation in the\n",
    "        color bar\n",
    "    \"\"\"\n",
    "    if scale == 'log':\n",
    "        norm = ImageNormalize(data_2d, interval=ManualInterval(vmin=vmin, vmax=vmax),\n",
    "                              stretch=LogStretch())\n",
    "    elif scale == 'linear':\n",
    "        norm = ImageNormalize(data_2d, interval=ManualInterval(vmin=vmin, vmax=vmax),\n",
    "                              stretch=LinearStretch())\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(data_2d, origin='lower', norm=norm)\n",
    "    \n",
    "    if xpixel and ypixel:\n",
    "        plt.plot(xpixel, ypixel, marker='o', color='red', label='Selected Pixel')\n",
    "\n",
    "    fig.colorbar(im, label=units)\n",
    "    plt.xlabel('Pixel column')\n",
    "    plt.ylabel('Pixel row')\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_load\"></a>\n",
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we will use calbrated rate files from a NIRCam simulated imaging exposure that is stored in Box. Let's download these files, as well as an association file and some parameter reference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.utils.data import download_file\n",
    "from pathlib import Path\n",
    "from shutil import move\n",
    "from os.path import splitext\n",
    "\n",
    "def get_box_files(file_list):\n",
    "    for box_url,file_name in file_list:\n",
    "        if 'https' not in box_url:\n",
    "            box_url = 'https://stsci.box.com/shared/static/' + box_url\n",
    "        downloaded_file = download_file(box_url, timeout=600)\n",
    "        if Path(file_name).suffix == '':\n",
    "            ext = splitext(box_url)[1]\n",
    "            file_name += ext\n",
    "        move(downloaded_file, file_name)\n",
    "        \n",
    "file_urls = ['https://stsci.box.com/shared/static/p2wlvndw25dk7xwk1tasqevmhkg6p55e.fits',\n",
    "             'https://stsci.box.com/shared/static/cmhc7kkf5z6373d2vwg7916lrhe5ia7u.fits',\n",
    "             'https://stsci.box.com/shared/static/sb18cpfqjbw1i09gvw0cpqkj6ymdn899.fits',\n",
    "             'https://stsci.box.com/shared/static/7d00b9isvss7njhcwmd8uiq8c7s4d845.json',\n",
    "             'https://stsci.box.com/shared/static/ja0gkd8c0x8p8konhr84wkuhwnpkqf4s.asdf',\n",
    "             'https://stsci.box.com/shared/static/yahdw55fotwrh7i6hhcxksj97qkf7j4r.asdf',\n",
    "              ]\n",
    "\n",
    "file_names = ['jw98765001001_01101_00001_nrcb5_cal.fits',\n",
    "              'jw98765001001_01101_00002_nrcb5_cal.fits',\n",
    "              'jw98765001001_01101_00003_nrcb5_cal.fits',\n",
    "              'level3_lw_asn.json',\n",
    "              'jwst_nircam_pars-tweakregstep_0006.asdf',\n",
    "              'nircam_pars-sourcecatalogstep_f444w_clear.asdf'\n",
    "              ]  \n",
    "\n",
    "box_download_list = [(url,name) for url,name in zip(file_urls,file_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_box_files(box_download_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_file = os.path.join(output_dir, 'level3_lw_asn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the association file and load into a json object\n",
    "with open(asn_file) as f_obj:\n",
    "    asn_data = json.load(f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the association file begins with a few lines of data that give high-level information about the association. The most important entry here is the `asn_rule` field. Association files have different formats for the different stages of the pipeline. You should be sure that the `asn_rule` matches the pipeline that you will be running. In this case we'll be running the Stage 3 pipeline, and we see that the `asn_rule` mentions \"Level3\", which is what we want.\n",
    "\n",
    "Beneath these lines, we see the `products` field. This field contains a list of dictionaries that specify the files that belong to this association, and the types of those files. When the Stage 3 pipeline is run on this association file, all files listed here will be run through the calibration steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweak_files = ['level3_lw_asn_0_tweakregstep.fits',\n",
    "               'level3_lw_asn_1_tweakregstep.fits',\n",
    "               'level3_lw_asn_2_tweakregstep.fits']\n",
    "tweak_product = 'manual_asn_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweakreg_asn = asn_from_list.asn_from_list(tweak_files, rule=DMS_Level3_Base, product_name=tweak_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our new association, containing the three files created by the `tweakreg` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweakreg_asn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the new association to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = 'manual_tweakreg_asn.json'\n",
    "with open(output_test, 'w') as outfile:\n",
    "    name, serialized = tweakreg_asn.dump(format='json')\n",
    "    outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='image3'></a>\n",
    "# The calwebb_image3 pipeline: Ensemble processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, we will run the Stage 3 pipeline using an association file containing several NIRCam exposures. We will first call the entire *calwebb_image3* pipeline itself. The pipeline is a wrapper which will string together all of the appropriate steps in the proper order.\n",
    "\n",
    "After running the entire pipeline, we will go back to the original calibrated slope images and manually run them through each of the steps that comprise the Stage 3 pipeline. For each step we will check in more detail what is going on and examine how the exposure files have changed.\n",
    "\n",
    "See [Figure 1](https://jwst-docs.stsci.edu/jwst-data-reduction-pipeline/algorithm-documentation/stages-of-processing/calwebb_image3) on the calwebb_image3 algorithm page for a map of the steps are performed on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image3_at_once'></a>\n",
    "# Run the entire `calwebb_image3` pipeline\n",
    "\n",
    "In this section we show how to run the entire calwebb_image3 pipeline with a single call. \n",
    "\n",
    "We set parameter values for some of the individual steps, save some outputs, etc, and then call the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the pipeline class\n",
    "image3 = calwebb_image3.Image3Pipeline()\n",
    "\n",
    "# Set some parameters that pertain to the\n",
    "# entire pipeline\n",
    "image3.output_dir = output_dir\n",
    "image3.save_results = True\n",
    "image3.tweakreg.save_results = True\n",
    "image3.skymatch.save_results = True\n",
    "image3.outlier_detection.save_results = True\n",
    "image3.resample.save_results = True\n",
    "image3.source_catalog.save_results = True\n",
    "\n",
    "# Set some parameters that pertain to some of\n",
    "# the individual steps\n",
    "image3.tweakreg.snr_threshold = 10.0  # 5.0 is the default\n",
    "image3.tweakreg.kernel_fwhm = 2.302  # 2.5 is the default\n",
    "image3.tweakreg.brightest = 20  # 100 is the default\n",
    "image3.tweakreg.align_to_gaia = True\n",
    "image3.tweakreg.save_gaia_catalog = True\n",
    "image3.source_catalog.kernel_fwhm = 2.302  # pixels\n",
    "image3.source_catalog.snr_threshold = 10.\n",
    "\n",
    "# Call the run() method\n",
    "image3.run(asn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the input filenames from the association file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [item['expname'] for item in asn_data['products'][0]['members']]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the names of the other output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_file = os.path.join(output_dir, 'l3_lw_results_i2d.fits')\n",
    "source_cat_file = os.path.join(output_dir, 'l3_lw_results_cat.ecsv')\n",
    "segmentation_map_file = os.path.join(output_dir, 'l3_lw_results_segm.fits')\n",
    "cr_flagged_files = [item.replace('cal.fits', 'crf.fits') for item in input_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the final mosaic image and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with datamodels.open(mosaic_file) as mosaic:\n",
    "mosaic = datamodels.open(mosaic_file)\n",
    "print(mosaic.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(mosaic.data, vmin=0, vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the segmentation map that was created by the `source_catalog` step. This shows which pixels are associated with the identified sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map = fits.getdata(segmentation_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image(seg_map, vmin=0, vmax=5, scale='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now examine the actual source catalog. For each source, the catalog lists the location, along with flux and AB/Vega magnitude values in three different apertures, as well as calculated values for an infinite aperture. Within the documentation, you can see the [full list of column definitions](https://jwst-pipeline.readthedocs.io/en/stable/jwst/source_catalog/main.html#source-catalog-table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cat = ascii.read(source_cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's overlay the source catalog on top of the mosaic image. In order to cut down on the number of spurious detections, we only show sources above a minimum flux limit. Another way to cut down on the number of spurious detections would be to change some of the `source_catalog` parameter values when calling the pipeline above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_catalog(mosaic.data, source_cat, flux_limit=5e-7, vmin=0, vmax=10,\n",
    "                title='Final mosaic with source catalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image3_step_by_step'></a>\n",
    "# Run the individual pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below we run the steps contained within calwebb_image3 one at a time, in order to check the outputs for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tweakreg'></a>\n",
    "### The `WCS refinement` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step, called the `tweakreg` step, mimics the behavior of the tweakreg step of Astrodrizzle. Given a series of images, it identifies point sources that are common to two or more images, and uses those sources' locations to correct the WCS of the input images. The tweaks are such that when the images are later combined into a final mosiac image, the WCS of the input images will align on the sky. \n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/tweakreg/README.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are many [optional input arguments](https://jwst-pipeline.readthedocs.io/en/stable/jwst/tweakreg/README.html#step-arguments).\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step does not use any reference files.\n",
    "\n",
    "#### Parameter reference files\n",
    "\n",
    "There are filter-dependent parameter reference files in CRDS for this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran the step above and saved the outputs, we'll skip this part and just load the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what parameters are available to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TweakRegStep.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create instance, set parameters, and run\n",
    "# tweakreg = TweakRegStep()\n",
    "# tweakreg.kernel_fwhm = 2.302   # Gaussian FWHM in pixels\n",
    "# tweakreg.snr_threshold = 10.0  # SNR threshold above background\n",
    "# tweakreg.brightest = 100       # Number of brightest objects to keep\n",
    "# tweakreg.save_results = True\n",
    "# tweak = tweakreg.run(asn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step saves the results in new fits files with updated WCS information. Let's look at the difference in the WCS before and after the step by loading the WCS objects, and calculating the RA, Dec at one pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_file = 'jw98765001001_01101_00003_nrcb5_cal.fits'\n",
    "tweak_file = 'jw98765001001_01101_00003_nrcb5_tweakreg.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files using datamodels\n",
    "#with datamodels.open(cal_file) as cal_data:\n",
    "cal_data = datamodels.open(cal_file)\n",
    "print(type(cal_data))\n",
    "#with datamodels.open(tweak_file) as tweak_data:\n",
    "tweak_data = datamodels.open(tweak_file)\n",
    "print(type(tweak_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_detector_to_world = cal_data.meta.wcs.get_transform('detector', 'world')\n",
    "run_detector_to_world = tweak_data.meta.wcs.get_transform('detector', 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at RA, Dec in the center of the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_detector_to_world(x, y))\n",
    "print(run_detector_to_world(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shift in the WCS before/after this step?\n",
    "delta_ra = run_detector_to_world(x, y)[0] - cal_detector_to_world(x, y)[0]\n",
    "delta_dec = run_detector_to_world(x, y)[1] - cal_detector_to_world(x, y)[1]\n",
    "print('Shift in RA, Dec is ({:.4f}, {:.4f}) arcsec'.format(delta_ra * 3600., delta_dec * 3600.))\n",
    "print('This is ({:.3f}, {:.3f}) pixels.'.format(delta_ra * 3600. / .062, delta_dec * 3600. / 0.062))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extremely small shift is expected, since the simulated data used in this exercise has no jitter or offsets added to it.\n",
    "\n",
    "Just for fun, let's look at the contents of the WCS object in one of the files saved by the tweakreg step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweak_data.meta.wcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='skymatch'></a>\n",
    "## The `Sky matching` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step calculates sky values in overlapping regions of the input images. Sky values can be computed for each image separately or in a way that matches the sky levels amongst the collection of images so as to minimize their differences.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/skymatch/README.html) of the step. Note that there are several possible methods for calculating the sky values. \n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are [several optional arguments](https://jwst-pipeline.readthedocs.io/en/stable/jwst/skymatch/README.html#step-arguments) for this step.\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step does not use any reference files.\n",
    "\n",
    "#### Parameter reference files\n",
    "There are currently no parameter reference files for this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran the step above and saved the outputs, we'll skip this part and just load the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        The easiest way to call this step is to supply the output object from the tweakreg call above. The other option would be to supply an association file. However, since the tweakreg step saved modified files, we would have to create a new association file that contains these new files, and then supply that association file in the call to skymatch. This pattern is the same for all of the steps in calwebb_image3, since all steps now expect association files for inputs, rather than individual files. \n",
    "\n",
    "        To create a new association file, follow the steps outlined in the [Creating your own association files](#diy_association) section above. Keep in mind that you could also simply make a copy of an existing association file and update the member filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see what parameters are available to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SkyMatchStep.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# skymatch = SkyMatchStep()\n",
    "# skymatch.skymethod = 'global+match' # this is the default. Set here as an example\n",
    "# skymatch.save_results = True\n",
    "# sky = skymatch.run(tweak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you request to save the output from this step, two new header keywords are added to the primary header of the outputs. In each file, the `BKGLEVEL` keyword lists the computed background level, and the `BKGSUB` keyword says whether or not the background has been subtracted from the data.\n",
    "\n",
    "In this case, we kept the default behavior of not subtracting the background from the data, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sky_file = 'step_SkyMatchStep_2_skymatchstep.fits'\n",
    "sky_file = 'l3_lw_results_2_skymatch.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_header = fits.getheader(sky_file, 0)\n",
    "print('Computed background level: {}'.format(sky_header['BKGLEVEL']))\n",
    "print('Background subtracted: {}'.format(sky_header['BKGSUB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the calculated background value is not subtracted from the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with datamodels.open(sky_file) as sky_data:\n",
    "sky_data = datamodels.open(sky_file)\n",
    "print(sky_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(sky_data.data - cal_data.data), np.max(sky_data.data - cal_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outlier_detection'></a>\n",
    "## The `Outlier Detection` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step uses the collection of input files to identify and flag any cosmic rays or other transient image artifacts that were not flagged by the jump step in calwebb_detector1. While the jump step looked for large pixel-based deviations in the signal from group-to-group within an integration, the outlier detection step looks for large sky-based exposure-to-exposure devations in the signal. If a given location on the sky shows no signal above the noise in 4 out of 5 exposures, but a bright source in the remaining exposure, the outlier detction step will flag in the DQ map the pixels containing the source in the fifth exposure. These pixels will be ignored when the exposures are later combined into a final mosaic image.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/outlier_detection/main.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are [numerous optional arguments](https://jwst-pipeline.readthedocs.io/en/stable/jwst/outlier_detection/arguments.html) for this step, including several that apply to the resample step, which this step makes use of.\n",
    "\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step does not use any reference files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran the step above and saved the outputs, we'll skip this part and just load the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the available parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OutlierDetectionStep.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Run the step\n",
    "# outlier_detection = OutlierDetectionStep()\n",
    "# outlier_detection.save_results = True\n",
    "# outlier = outlier_detection.run(sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_file = 'step_SkyMatchStep_2_a3001_outlierdetectionstep.fits'\n",
    "outlier_file = 'l3_lw_results_2_a3001_crf.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open using datamodels\n",
    "#with datamodels.open(outlier_file) as outlier_data:\n",
    "outlier_data = datamodels.open(outlier_file)\n",
    "print(outlier_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of pixels where DQ flags were changed\n",
    "new_flags = np.where(cal_data.dq != outlier_data.dq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found {} pixels with updated DQ flag values.\".format(len(new_flags[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the DQ value for one of these pixels before and after the outlier detection step has run, in order to see what has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12515\n",
    "y = new_flags[0][index]\n",
    "x = new_flags[1][index]\n",
    "print('Pixel x, y = ({}, {})'.format(x, y))\n",
    "print('Before: {}'.format(cal_data.dq[y, x]))\n",
    "print('After: {}'.format(outlier_data.dq[y, x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that the 'OUTLIER' and the 'DO_NOT_USE' flags are new. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_bad_pix_types(cal_data.dq[y, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_bad_pix_types(outlier_data.dq[y, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data itself remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_data.data[y, x], outlier_data.data[y, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Resample step is run next in order to create the final mosaic image, all pixels flagged as DO_NOT_USE will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resample'> </a>\n",
    "## The `Resample` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "We initially saw this step in calwebb_image2, where it was used to resample individual images onto a distortion-free pixel grid. This time, the Resample step works on the set of input images, which now all have a consistent WCS, thanks to the tweakreg step. The input images are combined into a final mosaic as they are resampled onto a distortion-free grid. The output of this step is the final image output of the Stage 3 pipeline. \n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/main.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There is a list of [optional Astrodrizzle-style](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/arguments.html) input parameters that can be used to customize the resampling process.\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step uses the [`DRIZPARS`](https://jwst-pipeline.readthedocs.io/en/stable/jwst/resample/reference_files.html) reference file. This file contains Astrodrizzle-style keywords that can be used to control the details of the resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran the step above and saved the outputs, we'll skip this part and just load the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the available parameters and their default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ResampleStep.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Run the step\n",
    "# resample = ResampleStep()\n",
    "# resample.save_results = True\n",
    "# resamp = resample.run(outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resamp_file = 'step_ResampleStep_resamplestep.fits'\n",
    "resamp_file = 'l3_lw_results_i2d.fits'\n",
    "\n",
    "#with datamodels.open(resamp_file) as resamp:\n",
    "resamp = datamodels.open(resamp_file)\n",
    "print(resamp.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_image(resamp.data, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='source_catalog'> </a>\n",
    "## The `Source Catalog` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step creates a catalog of source photometry and morphology information. Sources are identified using [Photutils' image segmentation](https://photutils.readthedocs.io/en/latest/segmentation.html) method. The output is an ASCII file containing a table of source locations and aperture photometry results.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/source_catalog/main.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There is a list of [optional input parameters](https://jwst-pipeline.readthedocs.io/en/stable/jwst/source_catalog/arguments.html) that can be used to customize the resampling process.\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step uses the [`APCORR` and `ABVEGAOFFSET`](https://jwst-pipeline.readthedocs.io/en/stable/jwst/source_catalog/reference_files.html) reference files. The `APCORR` reference file contains the factors necessary to correct aperture photometry results to the equivalent of an infinite aperure. The `ABVEGAOFFSET` reference file contains data necessary for converting from AB to Vega magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='srccat_run'></a>\n",
    "#### Run the step with the run() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran the step above and saved the outputs, we'll skip this part and just load the outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the available parameters and their default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SourceCatalogStep.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the step after setting some parameters\n",
    "srccat = SourceCatalogStep()\n",
    "srccat.save_results = True\n",
    "srccat.kernel_fwhm = 2.302  # pixels\n",
    "srccat.snr_threshold = 10.\n",
    "\n",
    "source_cat = srccat.run(resamp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overlay_catalog(resamp.data, source_cat, flux_limit=0, vmin=0, vmax=10,\n",
    "                title='Final mosaic with source catalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about_ID\"></a>\n",
    "## About this Notebook\n",
    "**Original Author:** Bryan Hilbert, updated by Alicia Canipe, NIRCam\n",
    "<br>**Updated On:** 07/28/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Notebook](#title_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
