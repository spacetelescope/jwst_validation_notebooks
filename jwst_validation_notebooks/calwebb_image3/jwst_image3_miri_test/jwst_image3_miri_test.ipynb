{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Notebook: calwebb_image3 with MIRI\n",
    "\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: MIRI, NIRCam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook processes a set of simulated images through the full pipeline, with emphasis on the calwebb_image3 pipeline.\n",
    "\n",
    "1) Read in images.\n",
    "\n",
    "2) Run images through the detector1 and image2 pipelines to get calibrated images.\n",
    "\n",
    "3) Set up association file.\n",
    "\n",
    "4) Run image3 pipeline with association file as input.\n",
    "\n",
    "5) Display and examine combined image.\n",
    "\n",
    "\n",
    "The pipeline documentation can be found here: https://jwst-pipeline.readthedocs.io/en/latest/\n",
    "\n",
    "The pipeline code is available on GitHub: https://github.com/spacetelescope/jwst\n",
    "\n",
    "Author: T. Temim and M. Cracraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "data_dir = TemporaryDirectory()\n",
    "os.chdir(data_dir.name)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'CRDS_CACHE_TYPE' in os.environ:\n",
    "    if os.environ['CRDS_CACHE_TYPE'] == 'local':\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'], 'crds', 'cache')\n",
    "    elif os.path.isdir(os.environ['CRDS_CACHE_TYPE']):\n",
    "        os.environ['CRDS_PATH'] = os.environ['CRDS_CACHE_TYPE']\n",
    "print('CRDS cache location: {}'.format(os.environ['CRDS_PATH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import jwst\n",
    "import json\n",
    "import glob\n",
    "#from jwst.pipeline import Image2Pipeline\n",
    "#from jwst.pipeline import Image3Pipeline\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.datamodels import ImageModel\n",
    "from jwst.pipeline import calwebb_image3\n",
    "from jwst.pipeline import calwebb_image2\n",
    "from jwst.pipeline import calwebb_detector1\n",
    "from jwst.pipeline import Detector1Pipeline, Image2Pipeline, Image3Pipeline\n",
    "from jwst.associations import asn_from_list\n",
    "from jwst import datamodels\n",
    "from matplotlib import rcParams\n",
    "from ci_watson.artifactory_helpers import get_bigdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print pipeline version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jwst.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data path and image list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading input files\")\n",
    " \n",
    "#This readnoise file is needed for use with simulated data which has higher readnoise than actual data.\n",
    "readnoise = get_bigdata('jwst_validation_notebooks',\n",
    "                     'validation_data',\n",
    "                     'jump',                     \n",
    "                     'jump_miri_test', \n",
    "                     'jwst_mirisim_readnoise.fits')\n",
    "\n",
    "filelist = ['starfield_50star4ptdither_seq1_MIRIMAGE_F1130Wexp1.fits', \n",
    "            'starfield_50star4ptdither_seq1_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq2_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq2_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq3_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq3_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq4_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq4_MIRIMAGE_F1130Wexp2.fits']\n",
    "\n",
    "for file in filelist:\n",
    "    input_file = get_bigdata('jwst_validation_notebooks',\n",
    "                     'validation_data',\n",
    "                     'outlier_detection',\n",
    "                     'outlier_detection_miri_test',\n",
    "                     file)\n",
    "\n",
    "print(\"Finished Downloads\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run files through detector one and Image2 pipelines to get most up to date calibrated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the calwebb_detector1 pipeline\n",
    "\n",
    "# set up pipeline parameters \n",
    "rej_thresh=10.0  # rejection threshold for jump step\n",
    "\n",
    "print('There are ', len(filelist), ' images.')\n",
    "    \n",
    "slopelist = []    \n",
    "    \n",
    "# loop over list of files\n",
    "for file in filelist:\n",
    "    \n",
    "    # set up pipeline parameters for input\n",
    "    pipe1 = Detector1Pipeline()\n",
    "    pipe1.jump.rejection_threshold = rej_thresh\n",
    "    pipe1.jump.override_readnoise = readnoise\n",
    "    pipe1.ramp_fit.override_readnoise = readnoise\n",
    "    pipe1.refpix.skip = True  # needs update to simulator for this to work properly with simulated data\n",
    "       \n",
    "    # set up output file name\n",
    "    base, remainder = file.split('.')\n",
    "    outname = base\n",
    "        \n",
    "    pipe1.jump.output_file = outname+'.fits'    \n",
    "    #pipe1.ramp_fit.output_file = outname+'.fits'\n",
    "    pipe1.output_file = outname+'.fits'\n",
    "\n",
    "    # Run pipeline on each file\n",
    "    rampfile = pipe1.run(file)\n",
    "    slopelist.append(rampfile)\n",
    "    \n",
    "    # Close the input files\n",
    "    #file.close()\n",
    "\n",
    "print('Detector 1 steps completed on all files.')\n",
    "print(slopelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Calwebb_image2 on output files from detector1\n",
    "    \n",
    "print('There are ', len(slopelist), ' images.')\n",
    "    \n",
    "callist = []\n",
    "\n",
    "# cycle through files\n",
    "for rampfile in slopelist:\n",
    "    \n",
    "    # create an object for the pipeline    \n",
    "    pipe2 = Image2Pipeline()\n",
    "\n",
    "    filename = rampfile.meta.filename\n",
    "    # Set pipeline parameters\n",
    "\n",
    "    pipe2.save_results = True\n",
    "    pipe2.output_file = filename +'_cal.fits'\n",
    "    pipe2.resample.save_results = True\n",
    "    pipe2.suffix = None\n",
    "\n",
    "    calfile = pipe2.run(rampfile)\n",
    "\n",
    "    callist.append(calfile)\n",
    "\n",
    "print(callist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up association files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_list = glob.glob('starfield*_cal.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use asn_from_list to create association table\n",
    "#cal_list=input_files\n",
    "asn = asn_from_list.asn_from_list(cal_list, rule=DMS_Level3_Base, product_name='image3_combined.fits')\n",
    "\n",
    "# use this if you need to add non'science' exposure types\n",
    "#asn['products'][0]['members'][1]['exptype'] = 'background'\n",
    "#asn['products'][0]['members'][2]['exptype'] = 'sourcecat'\n",
    "\n",
    "# dump association table to a .json file for use in image3\n",
    "with open('image3_asnfile.json', 'w') as fp:\n",
    "    fp.write(asn.dump()[1])\n",
    "\n",
    "image3_json_file='image3_asnfile.json'\n",
    "    \n",
    "json_file = image3_json_file\n",
    "file_list = []\n",
    "file_list2 = []\n",
    "with open(json_file) as json_data:\n",
    "    d = json.load(json_data)\n",
    "    members = d['products'][0]['members']\n",
    "    for item in np.arange(0,len(members)):\n",
    "        file_list.append(members[item]['expname'])\n",
    "        file_list2.append(members[item]['expname'][:-5]+\"_image3.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run input data through image3 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in parameters needed to give better source finding results\n",
    "\n",
    "# set any specific parameters\n",
    "# tweakreg parameters to allow data to run\n",
    "fwhm=3.762  # Gaussian kernel FWHM of objects expected, default=2.5\n",
    "minobj=5  # minimum number of objects needed to match positions for a good fit, default=15\n",
    "snr= 100 # signal to noise threshold, default=5\n",
    "sigma= 3 # clipping limit, in sigma units, used when performing fit, default=3\n",
    "fit_geom='shift' # ftype of affine transformation to be considered when fitting catalogs, default='general'\n",
    "use2dhist=False  # boolean indicating whether to use 2D histogram to find initial offset, default=True\n",
    "\n",
    "pipe3 = calwebb_image3.Image3Pipeline()    \n",
    "pipe3.tweakreg.kernel_fwhm = fwhm\n",
    "pipe3.tweakreg.snr_threshold = snr\n",
    "pipe3.tweakreg.minobj = minobj\n",
    "pipe3.tweakreg.sigma = sigma\n",
    "pipe3.tweakreg.fitgeometry = fit_geom\n",
    "pipe3.tweakreg.use2dhist = use2dhist\n",
    "pipe3.source_catalog.kernel_fwhm = fwhm\n",
    "pipe3.source_catalog.snr_threshold = snr\n",
    "pipe3.skymatch.save_results = True\n",
    "pipe3.outlier_detection.save_results = True\n",
    "pipe3.resample.save_results = True\n",
    "pipe3.source_catalog.save_results = True\n",
    "pipe3.save_results = True\n",
    "\n",
    "pipe3.run('image3_asnfile.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display and examine combined image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with datamodels.open('image3_combined_i2d.fits') as im_i2d:\n",
    "#    # raises exception if myimage.fits is not an image file\n",
    "#    pass\n",
    "\n",
    "im_i2d = ImageModel('image3_combined_i2d.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ecsv photom file\n",
    "from astropy.visualization import LogStretch, PercentileInterval, ManualInterval\n",
    "from astropy import table\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.visualization import (MinMaxInterval, SqrtStretch,\n",
    "                                   ImageNormalize)\n",
    "\n",
    "viz1 = LogStretch()\n",
    "viz2 = LogStretch() + ManualInterval(-12,100)\n",
    "norm = ImageNormalize(im_i2d.data, interval=MinMaxInterval(),\n",
    "                      stretch=SqrtStretch())\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "#plt.imshow(viz2(im_i2d.data),cmap='gray')\n",
    "plt.imshow(im_i2d.data,origin='lower',vmin=5, vmax=15)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display combined image with catalog sources overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photfile = 'image3_combined_cat.ecsv'\n",
    "data = table.Table.read(photfile, format='ascii', comment='#')\n",
    "print(len(data),' sources detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ecsv photom file\n",
    "from astropy.visualization import LogStretch, PercentileInterval, ManualInterval\n",
    "from astropy import table\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.visualization import (MinMaxInterval, SqrtStretch,\n",
    "                                   ImageNormalize)\n",
    "\n",
    "viz1 = LogStretch()\n",
    "viz2 = LogStretch() + ManualInterval(-12,100)\n",
    "norm = ImageNormalize(im_i2d.data, interval=MinMaxInterval(),\n",
    "                      stretch=SqrtStretch())\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "#plt.imshow(viz2(im_i2d.data),cmap='gray')\n",
    "plt.imshow(im_i2d.data,origin='lower',vmin=5, vmax=15)\n",
    "plt.colorbar()\n",
    "plt.scatter(data['xcentroid'], data['ycentroid'],lw=1, s=10,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
