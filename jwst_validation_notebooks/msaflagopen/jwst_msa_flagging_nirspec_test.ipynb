{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Testing Notebook: spec2, msa_flagging step\n",
    "\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: NIRSpec \n",
    "\n",
    "Tested on CV3 data\n",
    "\n",
    "### Table of Contents\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "<br> [Imports](#imports_ID) <br> [Introduction](#intro_ID) <br> [Testing Data Set](#data_ID) <br> [Run the JWST pipeline and msa_flagging validation tests](#pipeline_ID): [MOS test](#MOS), [IFU test](#IFU) <br> [About This Notebook](#about_ID)<br> [Results](#results) <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import shutil\n",
    "data_dir = TemporaryDirectory()\n",
    "\n",
    "os.chdir(data_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'CRDS_CACHE_TYPE' in os.environ:\n",
    "    if os.environ['CRDS_CACHE_TYPE'] == 'local':\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'], 'crds', 'cache')\n",
    "    elif os.path.isdir(os.environ['CRDS_CACHE_TYPE']):\n",
    "        os.environ['CRDS_PATH'] = os.environ['CRDS_CACHE_TYPE']\n",
    "print('CRDS cache location: {}'.format(os.environ['CRDS_PATH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports_ID\"></a>\n",
    "# Imports\n",
    "The library imports relevant to this notebook are aready taken care of by importing the NIRSpec Pipeline Testing Tool.\n",
    "\n",
    "* os for filepaths\n",
    "* subprocess for installing NPTT and moving output files\n",
    "* astropy.io for opening fits files\n",
    "* jwst.msaflagopen.msaflagopen_step is the pipeline step being tested\n",
    "\n",
    "NOTE: This notebook assumes that the pipeline version to be tested is already installed and its environment is activated.\n",
    "\n",
    "To be able to run this notebook you need to install NPTT (https://github.com/spacetelescope/nirspec_pipe_testing_tool). \n",
    "\n",
    "If the installation is successful, you will be able to import NPTT.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45177853-942e-4949-9e30-f544d70ef5f4"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import psutil\n",
    "from astropy.io import fits\n",
    "\n",
    "# Only print a DeprecationWarning the first time it shows up, not every time.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"once\", category=DeprecationWarning)\n",
    "    import jwst\n",
    "    from jwst.pipeline.calwebb_detector1 import Detector1Pipeline\n",
    "    from jwst.assign_wcs.assign_wcs_step import AssignWcsStep\n",
    "    from jwst.msaflagopen.msaflagopen_step import MSAFlagOpenStep\n",
    "    from jwst import datamodels\n",
    "\n",
    "# The latest version of NPTT is installed in the requirements text file at:\n",
    "# /jwst_validation_notebooks/environment.yml\n",
    "\n",
    "# import NPTT\n",
    "import nirspec_pipe_testing_tool as nptt\n",
    "\n",
    "# To get data from Artifactory\n",
    "from ci_watson.artifactory_helpers import get_bigdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the version used is the right one\n",
    "\n",
    "pipeline_version = jwst.__version__\n",
    "nptt_version = nptt.__version__\n",
    "\n",
    "print(\"Using jwst pipeline version: \", pipeline_version)\n",
    "print(\"Using NPTT version: \", nptt_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro_ID\"></a>\n",
    "# Test Description\n",
    "\n",
    "The test is a comparison of the result of our implementation of the MSA Failed Open Flagging step algorithm versus the pipeline's implementation.\n",
    "\n",
    "The overlap between the pixels flagged as being affected by failed open shutters are compared in regions large enough in the spectral direction to account for the non-repeatable motion of the grating wheel (a few pixels).\n",
    "\n",
    "For the test to be considered PASSED, the overlap between the results of the two methods should be greater than or equal to msa_flagging_threshold percent for all failed open shutters affecting more than 100 pixels.\n",
    "\n",
    "The code for these Multi Object Spectroscopy (MOS) and Integral Field Unit (IFU) tests can be obtained from: https://github.com/spacetelescope/nirspec_pipe_testing_tool/blob/master/nirspec_pipe_testing_tool/calwebb_spec2_pytests/auxiliary_code/msa_flagging_testing.py. This pipeline step and the associated pytests are skipped if data is Fixed Slits (FS) or Bright Object Time Series (BOTS).\n",
    "\n",
    "The input file is defined in the variable ```input_file``` (see section [Testing Data Set and Variable Setup](#data_ID)).\n",
    "\n",
    "Step description: https://jwst-pipeline.readthedocs.io/en/latest/jwst/msaflagopen/index.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst/msaflagopen\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "If the test **PASSED** this means that all slitlets or slices individually passed the test. However, if ony one individual slitlet (for MOS data) or slice (for IFU data) test failed, the whole test will be reported as **FAILED**.\n",
    "\n",
    "\n",
    "### Calibration WG Requested Algorithm: \n",
    "\n",
    "A short description and link to the page: https://outerspace.stsci.edu/display/JWSTCC/Vanilla+MSA+Failed+Open+Flagging\n",
    "\n",
    "\n",
    "### Defining Terms\n",
    "Acronymns used in this notebook:\n",
    "\n",
    "pipeline: calibration pipeline\n",
    "\n",
    "cal_detector1: calibration pipeline Stage 1, detector processing\n",
    "\n",
    "spec2: calibration pipeline spectroscopic Stage 2, spectroscopic processing\n",
    "\n",
    "NPTT: NIRSpec Pipeline Testing Tool\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline_ID\"></a>\n",
    "# Run the JWST pipeline and msa_flagging validation tests\n",
    "\n",
    "The pipeline can be run from the command line in two variants: full or per step.\n",
    "\n",
    "To run the spec2 pipeline in full use the command: \n",
    "\n",
    "$ strun jwst.pipeline.Spec2Pipeline jwtest_rate.fits\n",
    "\n",
    "where jwtest_rate.fits is the output of cal_detector1.\n",
    "\n",
    "To only run the msa_flagging step, use the command:\n",
    "\n",
    "$ strun jwst.msaflagopen.msaflagopen_step jwtest_assign_wcs.fits\n",
    "\n",
    "where jwtest_assign_wcs.fits is the output of the previous step, assign_wcs.\n",
    "\n",
    "NIRSpec TA data will be run through the calwebb_detector1 and the imaging2 pipelines. The imaging pipeline can be run with the following command:\n",
    "\n",
    "$ strun jwst.pipeline.Image2Pipeline jwtest_rate.fits\n",
    "\n",
    "These options are also callable from a script with the testing environment active. The Python call for running the pipeline in full or by step are:\n",
    "\n",
    "$\\gt$ from jwst.pipeline.calwebb_spec2 import Spec2Pipeline\n",
    "\n",
    "$\\gt$ Spec2Pipeline.call(jwtest_rate.fits)\n",
    " \n",
    "or\n",
    " \n",
    "$\\gt$ from jwst.msaflagopen.msaflagopen_step import msaflagopen_step\n",
    " \n",
    "$\\gt$ msaflagopen_step.call(jwtest_rate.fits)\n",
    "\n",
    "For the imaging pipeline the call would be as follows:\n",
    "\n",
    "$\\gt$ from jwst.pipeline.calwebb_image2 import Image2Pipeline\n",
    "\n",
    "$\\gt$ Image2Pipeline.call(jwtest_rate.fits)\n",
    "\n",
    "NPTT can run the spec2 pipeline either in full or per step, as well as the imaging pipeline in full. In this notebook we will use NPTT to run the pipeline and the validation tests. To run NPTT, follow the directions in the corresponding repo page.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_ID\"></a>\n",
    "# Testing Data Set\n",
    "\n",
    "All testing data is from the CV3 campaign. We chose these files because this is our most complete data set, i.e. all modes and filter-grating combinations.\n",
    "\n",
    "Data used for testing:\n",
    "- MOS_G140M_LINE1\n",
    "- MOS_PRISM_CLEAR\n",
    "- IFU_G395H_F290LP\n",
    "- IFU_PRISM_CLEAR\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = {                \n",
    "                'mos_prism_clear':{\n",
    "                                  'uncal_file_nrs1': 'mos_prism_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'mos_prism_nrs2_uncal.fits',\n",
    "                                  'msa_shutter_config': 'V0030006000104_msa.fits' },\n",
    "                \n",
    "                'mos_g140m_f100lp':{\n",
    "                                  'uncal_file_nrs1': 'mos_g140m_line1_NRS1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'mos_g140m_line1_NRS2_uncal.fits',  \n",
    "                                  'msa_shutter_config': 'V8460001000101_msa.fits' },\n",
    "                \n",
    "                'ifu_prism_clear':{\n",
    "                                  'uncal_file_nrs1': 'ifu_prism_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'ifu_prism_nrs2_uncal.fits',\n",
    "                                  'msa_shutter_config': None },\n",
    "\n",
    "                'ifu_g395h_f290lp':{\n",
    "                                  'uncal_file_nrs1': 'ifu_g395h_f290lp_nrs1_uncal.fits',\n",
    "                                  'uncal_file_nrs2': 'ifu_g395h_f290lp_nrs2_uncal.fits',\n",
    "                                  'msa_shutter_config': None }\n",
    "\n",
    "               }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to pull data from Artifactory\n",
    "def get_artifactory_file(data_set_dict, detector):\n",
    "    \"\"\"This function creates a list with all the files needed per detector to run the test.\n",
    "    Args:\n",
    "        data_set_dict: dictionary, contains inputs for a specific mode and configuration\n",
    "        detector: string, either nrs1 or nrs2\n",
    "    Returns:\n",
    "        data: list, contains all files needed to run test\n",
    "    \"\"\"\n",
    "    files2obtain = ['uncal_file_nrs1', 'msa_shutter_config']\n",
    "    data = []\n",
    "    for file in files2obtain:\n",
    "        data_file = None\n",
    "        try: \n",
    "            if '_nrs' in file and '2' in detector:\n",
    "                file = file.replace('_nrs1', '_nrs2')\n",
    "\n",
    "            data_file = get_bigdata('jwst_validation_notebooks',\n",
    "                                         'validation_data',\n",
    "                                         'nirspec_data', \n",
    "                                         data_set_dict[file])\n",
    "        except TypeError:\n",
    "            data.append(None)\n",
    "            continue\n",
    "\n",
    "        data.append(data_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common NPTT switches for this test\n",
    "\n",
    "# accepted threshold difference with respect to benchmark files\n",
    "msa_flagging_threshold = 99.5\n",
    "\n",
    "# other NPTT variables\n",
    "stellarity = None\n",
    "operability_ref = None\n",
    "source_type = None\n",
    "save_figs = False\n",
    "show_figs = True\n",
    "debug = False\n",
    "verbose = False\n",
    "\n",
    "# Get the data\n",
    "results_dict = {}\n",
    "detectors = ['nrs1', 'nrs2']\n",
    "for mode_config, data_set_dict in testing_data.items():\n",
    "    for det in detectors:\n",
    "        print('Testing files for detector: ', det)\n",
    "        data = get_artifactory_file(data_set_dict, det)\n",
    "        uncal_file, msa_shutter_config = data\n",
    "        print('Working with uncal_file: ', uncal_file)\n",
    "        uncal_basename = os.path.basename(uncal_file)\n",
    "                \n",
    "        # Run the stage 1 pipeline \n",
    "        rate_object = Detector1Pipeline.call(uncal_file)\n",
    "        # Make sure the MSA shutter configuration file is set up correctly\n",
    "        if msa_shutter_config is not None:\n",
    "            msa_metadata = rate_object.meta.instrument.msa_metadata_file\n",
    "            print(msa_metadata)\n",
    "            if msa_metadata is None or msa_metadata == 'N/A':\n",
    "                rate_object.meta.instrument.msa_metadata_file = msa_shutter_config\n",
    "\n",
    "        # Run the stage 2 pipeline steps\n",
    "        try:\n",
    "            pipe_object = AssignWcsStep.call(rate_object)\n",
    "            skip_file = False\n",
    "        except:\n",
    "            print(\"An error occured with AssignWcs. Likely: No open slits fall on detector\", det)\n",
    "            print(\"Skipping test for this file. \\n\")\n",
    "            skip_file = True\n",
    "       \n",
    "        if not skip_file:\n",
    "            msa_flagging_object = MSAFlagOpenStep.call(pipe_object)\n",
    "\n",
    "            # Run the validation test\n",
    "            %matplotlib inline\n",
    "\n",
    "            result, result_msg, log_msgs = nptt.calwebb_spec2_pytests.auxiliary_code.msa_flagging_testing.run_msa_flagging_testing(\n",
    "                                                                        msa_flagging_object, \n",
    "                                                                        msa_flagging_threshold = msa_flagging_threshold,\n",
    "                                                                        rate_obj = rate_object,\n",
    "                                                                        stellarity = stellarity, \n",
    "                                                                        operability_ref = operability_ref, \n",
    "                                                                        save_figs = save_figs, \n",
    "                                                                        show_figs = show_figs, \n",
    "                                                                        source_type = source_type, \n",
    "                                                                        debug = debug)\n",
    "\n",
    "        else:\n",
    "            result = 'skipped'\n",
    "\n",
    "\n",
    "        # Did the test pass\n",
    "        print(\"Did msa_flagging validation test pass? \", result, \"\\n\\n\")\n",
    "        rd = {uncal_basename: result}\n",
    "        results_dict.update(rd)\n",
    "        \n",
    "        # close all open files\n",
    "        psutil.Process().open_files()\n",
    "        closing_files = []\n",
    "        for fd in psutil.Process().open_files():\n",
    "            if data_dir.name in fd.path:\n",
    "                closing_files.append(fd)\n",
    "        for fd in closing_files:\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print('Closing file: ', fd)\n",
    "                open(fd.fd).close()\n",
    "            except:\n",
    "                if verbose:\n",
    "                    print('File already closed: ', fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly see if the test passed: Do the NIRSpec implementation and the pipeline's agree within <= 99.5%?\n",
    "\n",
    "print('These are the final results of the tests: ')\n",
    "for key, val in results_dict.items():\n",
    "    if not isinstance(val, str):\n",
    "        if val:\n",
    "            val = 'PASSED'\n",
    "        else:\n",
    "            val = 'FAILED'\n",
    "    print('{:<40} {:<8}'.format(key, val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"about_ID\"></a>\n",
    "## About this Notebook\n",
    "**Authors:** \n",
    "- Maria A. Pena-Guerrero, Staff Scientist II - Systems Science Support, NIRSpec\n",
    "- Emily Wislowski, Science Support Analyst I, NIRSpec\n",
    "\n",
    "<br>**Updated On:** May/25/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)\n",
    "<img style=\"float: right;\" src=\"./stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"stsci_pri_combo_mark_horizonal_white_bkgd\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
