{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Testing Notebook: Calwebb_Image3, Resample step with MIRI imaging\n",
    "\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: FGS, MIRI, NIRCam, NIRISS, NIRSpec \n",
    "\n",
    "Tested on MIRI Simulated data\n",
    "\n",
    "### Table of Contents\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "<br>  [Introduction](#intro_ID) <br> [Run JWST Pipelines](#pipeline_ID) <br> [Imports](#imports_ID) <br> [Create an association table for your cal files and run them through calwebb_image3](#runpipeline_ID) <br> [Find Stars in Image and Determine their Coordinates](#runscript_ID) <br> [Compare RA and Dec to expected Values](#residual_ID) <br> [About This Notebook](#about_ID) <br>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro_ID\"></a>\n",
    "# Introduction\n",
    "\n",
    "\n",
    "This test is designed to test the resample step in the calwebb_image3 pipeline. At the end of the calwebb_image3 pipeline, the set of files defined in an association table will be distortion corrected and combined. Resample is the step that applies the distortion correction using the drizzling algorithm (as defined in the DrizzlePac handbook) and combines the listed files. For more information on the pipeline step visit the links below. \n",
    "\n",
    "Step description: https://jwst-pipeline.readthedocs.io/en/latest/jwst/resample/main.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst/resample\n",
    "\n",
    "The data for this test were created with the MIRI Data Simulator, and the documentation for that code can be found here: http://miri.ster.kuleuven.be/bin/view/Public/MIRISim_Public\n",
    "\n",
    "\n",
    "### Calibration WG Requested Algorithm: \n",
    "\n",
    "A short description and link to the page: https://outerspace.stsci.edu/display/JWSTCC/Vanilla+Image+Combination\n",
    "\n",
    "\n",
    "### Defining Terms\n",
    "Definition of terms or acronymns.\n",
    "\n",
    "JWST: James Webb Space Telescope\n",
    "\n",
    "MIRI: Mid-Infrared Instrument\n",
    "\n",
    "MIRISim: MIRI Data Simulator\n",
    "\n",
    "### Description of test\n",
    "\n",
    "This test is performed by creating a set of simulated data with multiple point sources located at specified coordinates. The simulator puts in the expected distortion, so the initial output data comes out of the simulator in distorted coordinates. When this data is then run through calwebb_detector1, calwebb_image2 and calwebbb_image3, the combined, undistorted image should have the point sources registered at the expected locations. In flight, this test can be repeated with known stars that should be found at their expected coordinates. This notebook also checks that flux values for simulated data with roughly equivalent input values show no systematic patterns after resampling. This portion of the test will only work with simulated data created for that purpose.\n",
    "\n",
    "### Create the data for testing\n",
    "\n",
    "The set of data used in this particular test were created with the MIRI Data Simulator (MIRISim). Referring to the MIRISim link, you can see how to set up and run the simulator to re-create the input files if you wish. The data was run with a scene.ini file that specified what the scene should look like, with coordinates for the stars given in units of arcsecond offsets from the center of the field of view. The scene.ini file as well as the setup files simuation.ini and simulator.ini are needed to run the simulation.\n",
    "\n",
    "Once in the mirisim conda environment, the simulation is run with the command line:\n",
    "> mirisim simulation.ini\n",
    "\n",
    "The simulator created four files, two exposures each at two different dither positions, using the specified filter. Make sure the WCSAXES header keyword in the SCI extension is set to 2 and not 4. If it is set to 4, change it to 2.\n",
    "\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline_ID\"></a>\n",
    "## Run JWST Pipelines\n",
    "\n",
    "The four files were then run individually through the calwebb_detector1 and calwebb_image2 pipelines. When running the calwebb_detector1 pipeline, increase the threshold for a detection in the jump step from 4 sigma to 10 sigma to avoid a current issue where the jump detection step flags a large percentage of pixels as jumps. This can be done on the command line. (commands to be typed start with $)\n",
    "\n",
    "The pipelines can be run on the command line with the following commands or put into a script while using the pipeline conda environment.\n",
    "\n",
    "$ strun calwebb_detector1.cfg filename --steps.jump.rejection_threshold 10.0\n",
    "\n",
    "The output of the calwebb_detector1 pipeline is a set of four *rate.fits files which will then be run through the calwebb_image2 pipeline.\n",
    "\n",
    "$ strun calwebb_image2.cfg filename\n",
    "\n",
    "The output of the calwebb_image2 pipeline was then a set of four *cal.fits files. An association table was created that included these four files as input, and then the files and the association table were run through the calwebb_image3 pipeline. \n",
    "\n",
    "The cal files are stored in artifactory, and this notebook is meant to pull those files for the test of resample. Step through the cells of this notebook to run calwebb_image3 and then check the alignment.\n",
    "\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "data_dir = TemporaryDirectory()\n",
    "os.chdir(data_dir.name)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'CRDS_CACHE_TYPE' in os.environ:\n",
    "    if os.environ['CRDS_CACHE_TYPE'] == 'local':\n",
    "        os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'], 'crds', 'cache')\n",
    "    elif os.path.isdir(os.environ['CRDS_CACHE_TYPE']):\n",
    "        os.environ['CRDS_PATH'] = os.environ['CRDS_CACHE_TYPE']\n",
    "print('CRDS cache location: {}'.format(os.environ['CRDS_PATH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"imports_ID\"></a>\n",
    "# Imports\n",
    "The following packages will need to be imported for the scripts to work.\n",
    "\n",
    "\n",
    "* astropy.io for opening files\n",
    "* astropy.stats for sigma clipping routine\n",
    "* astropy.visualization for image plotting\n",
    "* ci_watson.artifactory_helpers to read in data from artifactory\n",
    "* jwst.datamodels for opening files as a JWST Datamodel\n",
    "* jwst.pipeline to run the pipeline step/module\n",
    "* jwst.associations to create association table\n",
    "* numpy for calculations\n",
    "* matplotlib.pyplot.plt to generate plot\n",
    "* os for path information  \n",
    "* photutils for star finding and aperture photometry\n",
    "* regtest to retrieve data from artifactory needed to run notebook\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45177853-942e-4949-9e30-f544d70ef5f4"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.io import ascii, fits\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.table import Column\n",
    "from astropy.visualization import SqrtStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from ci_watson.artifactory_helpers import get_bigdata\n",
    "import glob\n",
    "from itertools import product\n",
    "from jwst.datamodels import DrizProductModel, ImageModel\n",
    "from jwst.pipeline import Detector1Pipeline, Image2Pipeline, Image3Pipeline\n",
    "from jwst import associations\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.associations import asn_from_list\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from photutils import CircularAperture, DAOStarFinder, CircularAnnulus, aperture_photometry\n",
    "from jwst.regtest.regtestdata import RegtestData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in uncal data from artifactory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading input files\")\n",
    " \n",
    "#This readnoise file is needed for use with simulated data which has higher readnoise than actual data.\n",
    "readnoise = get_bigdata('jwst_validation_notebooks',\n",
    "                     'validation_data',\n",
    "                     'jump',                     \n",
    "                     'jump_miri_test', \n",
    "                     'jwst_mirisim_readnoise.fits')\n",
    "\n",
    "filelist = ['starfield_50star4ptdither_seq1_MIRIMAGE_F1130Wexp1.fits', \n",
    "            'starfield_50star4ptdither_seq1_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq2_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq2_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq3_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq3_MIRIMAGE_F1130Wexp2.fits',\n",
    "            'starfield_50star4ptdither_seq4_MIRIMAGE_F1130Wexp1.fits',\n",
    "            'starfield_50star4ptdither_seq4_MIRIMAGE_F1130Wexp2.fits']\n",
    "\n",
    "for file in filelist:\n",
    "    input_file = get_bigdata('jwst_validation_notebooks',\n",
    "                     'validation_data',\n",
    "                     'outlier_detection',\n",
    "                     'outlier_detection_miri_test',\n",
    "                     file)\n",
    "\n",
    "print(\"Finished Downloads\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Calwebb_detector1 to process files to ramp fit (slope) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the calwebb_detector1 pipeline\n",
    "\n",
    "# set up pipeline parameters \n",
    "rej_thresh=10.0  # rejection threshold for jump step\n",
    "\n",
    "print('There are ', len(filelist), ' images.')\n",
    "    \n",
    "slopelist = []    \n",
    "    \n",
    "# loop over list of files\n",
    "for file in filelist:\n",
    "    \n",
    "    # set up pipeline parameters for input\n",
    "    pipe1 = Detector1Pipeline()\n",
    "    pipe1.jump.rejection_threshold = rej_thresh\n",
    "    pipe1.jump.override_readnoise = readnoise\n",
    "    pipe1.ramp_fit.override_readnoise = readnoise\n",
    "    pipe1.refpix.skip = True  # needs update to simulator for this to work properly with simulated data\n",
    "       \n",
    "    # set up output file name\n",
    "    base, remainder = file.split('.')\n",
    "    outname = base\n",
    "        \n",
    "    pipe1.jump.output_file = outname+'.fits'    \n",
    "    #pipe1.ramp_fit.output_file = outname+'.fits'\n",
    "    pipe1.output_file = outname+'.fits'\n",
    "\n",
    "    # Run pipeline on each file\n",
    "    rampfile = pipe1.run(file)\n",
    "    slopelist.append(rampfile)\n",
    "    \n",
    "    # Close the input files\n",
    "    #file.close()\n",
    "\n",
    "print('Detector 1 steps completed on all files.')\n",
    "print(slopelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Calwebb_image2 on files output from detector1 to create calibrated individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Calwebb_image2 on output files from detector1\n",
    "    \n",
    "print('There are ', len(slopelist), ' images.')\n",
    "    \n",
    "callist = []\n",
    "\n",
    "# cycle through files\n",
    "for rampfile in slopelist:\n",
    "    \n",
    "    # create an object for the pipeline    \n",
    "    pipe2 = Image2Pipeline()\n",
    "\n",
    "    filename = rampfile.meta.filename\n",
    "    # Set pipeline parameters\n",
    "\n",
    "    pipe2.save_results = True\n",
    "    pipe2.output_file = filename +'_cal.fits'\n",
    "    pipe2.resample.save_results = True\n",
    "    pipe2.suffix = None\n",
    "\n",
    "    calfile = pipe2.run(rampfile)\n",
    "\n",
    "    callist.append(calfile)\n",
    "\n",
    "print(callist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an association table for the cal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use asn_from_list to create association table\n",
    "\n",
    "calfiles = glob.glob('starfield*_cal.fits')\n",
    "asn = asn_from_list.asn_from_list(calfiles, rule=DMS_Level3_Base, product_name='starfield_50star4ptdither_combined.fits')\n",
    "\n",
    "# use this if you need to add non'science' exposure types\n",
    "#asn['products'][0]['members'][1]['exptype'] = 'background'\n",
    "#asn['products'][0]['members'][2]['exptype'] = 'sourcecat'\n",
    "\n",
    "# dump association table to a .json file for use in image3\n",
    "with open('starfield_50star4ptdither_asnfile.json', 'w') as fp:\n",
    "    fp.write(asn.dump()[1])\n",
    "\n",
    "print(asn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"runpipeline_ID\"></a>\n",
    "# Use an association table for your cal files and run them through calwebb_image3\n",
    "\n",
    "Use the association table to process the .cal files that were output from calwebb_image2. That will be the input for calwebb_image3 that uses the resample step to combine each of the individual images.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regtest infrastructure to access all input files associated with the association file\n",
    "\n",
    "#rtdata = RegtestData(inputs_root=\"jwst_validation_notebooks\", env=\"validation_data\")\n",
    "#rtdata.get_asn(\"resample/resample_miri_test/starfield_50star4ptdither_771_asnfile.json\")\n",
    "#rtdata.input #this should be the list of files associated with the asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Calwebb_image3 on the association table\n",
    "    \n",
    "# set any specific parameters\n",
    "# tweakreg parameters to allow data to run\n",
    "fwhm = 3.318 #3.27  # Gaussian kernel FWHM of objects expected, default=2.5\n",
    "minobj = 5  # minimum number of objects needed to match positions for a good fit, default=15\n",
    "snr = 50 # signal to noise threshold, default=5\n",
    "sigma = 3 # clipping limit, in sigma units, used when performing fit, default=3\n",
    "fit_geom ='shift' # ftype of affine transformation to be considered when fitting catalogs, default='general'\n",
    "use2dhist = False  # boolean indicating whether to use 2D histogram to find initial offset, default=True\n",
    "   \n",
    "pipe3=Image3Pipeline()    \n",
    "pipe3.tweakreg.kernel_fwhm = fwhm\n",
    "pipe3.tweakreg.snr_threshold = snr\n",
    "pipe3.tweakreg.minobj = minobj\n",
    "pipe3.tweakreg.sigma = sigma\n",
    "pipe3.tweakreg.fitgeometry = fit_geom\n",
    "pipe3.tweakreg.use2dhist = use2dhist\n",
    "#pipe3.tweakreg.skip = True        # test to see if this affects the final output\n",
    "pipe3.source_catalog.save_results = True\n",
    "pipe3.save_results = True\n",
    "\n",
    "# run Image3\n",
    "\n",
    "#im = pipe3.run(rtdata.input)\n",
    "image = pipe3.run('starfield_50star4ptdither_asnfile.json')\n",
    "print('Image 3 pipeline finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"runscript_ID\"></a>\n",
    "# Find stars in image and determine their coordinates\n",
    "\n",
    "The output of the pipeline command in the previous step (given our association table) is an i2d.fits file. This file is in the format of a JWST Data model type of DrizProductModel and should be opened as such. It is this file that we will use for source finding and to determine whether the stars are found in the expected locations. The i2d file and the associated text file containing the input coordinates of the stars can be found in artifactory.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in combined i2d data file and list of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the combined data file and list of coordinates\n",
    "\n",
    "im = ImageModel('starfield_50star4ptdither_combined_i2d.fits')\n",
    "pixarea = im.meta.photometry.pixelarea_steradians\n",
    "print('Pixel area in steradians', pixarea)\n",
    "\n",
    "coords = get_bigdata('jwst_validation_notebooks',\n",
    "                     'validation_data',\n",
    "                     'resample',\n",
    "                     'resample_miri_test', \n",
    "                     'radec_4ptdith_50star_mosaic_coords.txt')\n",
    "\n",
    "# read in text file with RA and Dec input coordinates\n",
    "RA_in, Dec_in = np.loadtxt( coords, dtype=str, unpack=True)\n",
    "\n",
    "# put RA and Dec into floats\n",
    "RA_sim = RA_in.astype(float)\n",
    "Dec_sim = Dec_in.astype(float)\n",
    "\n",
    "\n",
    "# pull out data portion of input file\n",
    "data = im.data\n",
    "\n",
    "# print stats on input image\n",
    "mean, median, std = sigma_clipped_stats(data, sigma=200.0, maxiters=5)  # default sigma=3\n",
    "print('Image mean, median and std',mean, median, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run DAOStar finder to find sources in the image and examine the image and positions marked. \n",
    "The block of code below will find the sources in the image, create apertures for each source found, and output the table of x, y coordinates along with the peak pixel value. It will also show a scaled version of the image and mark in blue the positions of sources found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DAOStarFinder to find sources in image\n",
    "\n",
    "ap_radius = 5.  # radius for aperture for centroiding and photometry\n",
    "\n",
    "daofind = DAOStarFinder(fwhm=3.0, threshold=10.*std)    # default threshold=5*std, fwhm=3\n",
    "sources = daofind(data)    \n",
    "\n",
    "sources.pprint_all()\n",
    "#print(sources['xcentroid','ycentroid','peak'])   \n",
    "\n",
    "# Create apertures for x,y positions\n",
    "positions = tuple(zip(sources['xcentroid'], sources['ycentroid']))\n",
    "#print(positions)\n",
    "\n",
    "#positions = (sources['xcentroid'], sources['ycentroid'])\n",
    "apertures = CircularAperture(positions, r=ap_radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark sources on image frame to see if the correct sources were found\n",
    "norm = ImageNormalize(stretch=SqrtStretch())\n",
    "# keep image stretch in mind for plotting. sky subtracted range ~ (-15, 10), single sample ~ (0, 20)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(data, cmap='Greys', origin='lower', vmin=8,vmax=15)#, norm=norm)\n",
    "apertures.plot(color='red', lw=2.5) #, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run photometry on apertures (with a specified annulus for background subtraction)\n",
    "\n",
    "Set a specified annulus (inner and outer radii for the annulus).\n",
    "\n",
    "Run photometry on aperture and annuli.\n",
    "\n",
    "Subtract background values in annulus from aperture photometry.\n",
    "\n",
    "Output should be a table of photometry values printed to the screen (full table has columns id, xcenter, ycenter, aperture_sum and the added columns annulus_median, aperture_bkg and aperture_sum_bkgsub). You can choose which columns you wish to see printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for inner and outer annuli to collect background counts\n",
    "\n",
    "inner_annulus = 10.\n",
    "outer_annulus = 15.\n",
    "\n",
    "# set up annulus for background\n",
    "background_aper = CircularAnnulus(positions, r_in=inner_annulus, r_out=outer_annulus)\n",
    "\n",
    "# perform photometry on apertures for targets and background annuli\n",
    "phot_table = aperture_photometry(im.data, apertures)\n",
    "\n",
    "# perform background subtraction with outlier rejection\n",
    "bkg_median = []\n",
    "bkg_mask = background_aper.to_mask(method='center')\n",
    "bmask = bkg_mask[0]\n",
    "for mask in bkg_mask:\n",
    "    aper_data = bmask.multiply(data)\n",
    "    aper_data = aper_data[mask.data > 0]\n",
    "    \n",
    "    # perform sigma-clipped median\n",
    "    _, median_sigclip, _ = sigma_clipped_stats(aper_data)\n",
    "    bkg_median.append(median_sigclip)\n",
    "bkg_median = np.array(bkg_median)\n",
    "\n",
    "\n",
    "# do calculations on background regions found in annuli\n",
    "# Get average background per pixel\n",
    "phot_table['annulus_median'] = bkg_median\n",
    "# Get total background in the science aperture (per pixel * area in aperture)\n",
    "phot_table['aperture_bkg'] = bkg_median * apertures.area\n",
    "# subtract background in aperture from flux in aperture\n",
    "phot_table['aperture_sum_bkgsub'] = phot_table['aperture_sum'] - phot_table['aperture_bkg']\n",
    "\n",
    "# put aperture sum in pixel scale\n",
    "phot_table['scaled_ap_sum_bkgsub'] = phot_table['aperture_sum_bkgsub'] * pixarea\n",
    "\n",
    "#print(phot_table['aperture_sum','annulus_median','aperture_bkg','aperture_sum_bkgsub','scaled_ap_sum_bkgsub'])\n",
    "\n",
    "phot_table_sub = phot_table['aperture_sum','annulus_median','aperture_bkg','aperture_sum_bkgsub','scaled_ap_sum_bkgsub']\n",
    "phot_table_sub.pprint_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put x, y coordinates into RA and Dec using the wcs information from the files.\n",
    "The output of the next block of code should be a table showing the x and y centroid positions as well as the associated RA and Dec values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using wcs info from images, put coordinates into RA, Dec\n",
    "ra, dec = im.meta.wcs(sources['xcentroid'], sources['ycentroid'])\n",
    "\n",
    "# add RA, Dec to sources table\n",
    "\n",
    "ra_col = Column(name='RA', data=ra)\n",
    "dec_col = Column(name='Dec', data=dec)\n",
    "sources.add_column(ra_col)\n",
    "sources.add_column(dec_col)\n",
    "\n",
    "# print RA, Dec for each x, y position found\n",
    "#print(sources['xcentroid', 'ycentroid', 'RA', 'Dec'])  \n",
    "\n",
    "sources_sub = sources['xcentroid', 'ycentroid', 'RA', 'Dec']\n",
    "sources_sub.pprint_all()\n",
    "\n",
    "# add option to print out list of sources with flux values\n",
    "outtable = 'sourcelist_phot_rate.txt'\n",
    "sources.add_column(phot_table['aperture_sum'])\n",
    "sources.add_column(phot_table['aperture_sum_bkgsub'])\n",
    "sources.add_column(phot_table['scaled_ap_sum_bkgsub'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the RA and Dec positions used to create the simulated data to the values found in the output image.\n",
    "Difference each set of RA and Dec coordinates in both the input list and the found coordinates, taking into account any angles close to 360/0 degrees. If the difference for both the RA and Dec are below a set tolerance, then the positions match. Take the matched positions and convert the differences from degrees to milli arcseconds, and output the RA and Dec positions as well as the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare input RA, Dec to found RA, Dec\n",
    "print('       RA found       Dec found    RA_Diff (mas)  Dec_diff (mas)  Bkg sub flux  pass/fail')\n",
    "\n",
    "deltara = []\n",
    "deltadec = []\n",
    "\n",
    "for i in np.arange(0,len(RA_sim)):\n",
    "    for j in np.arange(0,len(ra)):\n",
    "        ra_diff = 180 - abs(abs(RA_sim[i] - ra[j])-180)\n",
    "        dec_diff = 180 - abs(abs(Dec_sim[i] - dec[j])-180)\n",
    "\n",
    "        if ra_diff < 1e-5 and dec_diff < 1e-5:\n",
    "            # put differences in milliarcseconds\n",
    "            ra_diff = ra_diff * 3600000\n",
    "            dec_diff = dec_diff * 3600000\n",
    "            deltara.append(ra_diff)\n",
    "            deltadec.append(dec_diff)\n",
    "\n",
    "            if ra_diff < 30 and dec_diff < 30: \n",
    "                test = 'pass' \n",
    "            else: \n",
    "                test = 'fail'\n",
    "            print('{:15.6f} {:15.6f} {:15.6f} {:15.6f} {:15.5e} {}'.format(ra[j], dec[j], ra_diff, dec_diff, \n",
    "                                                                        phot_table['scaled_ap_sum_bkgsub'][j], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ra and dec differences\n",
    "plt.title ('Differences in RA and Dec in millarcseconds')\n",
    "plt.ylabel('Delta RA')\n",
    "plt.xlabel('Delta Dec')\n",
    "plt.scatter(deltadec,deltara)\n",
    "plt.show()\n",
    "\n",
    "# Plot should show no differences greater than 30 milliarcseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"residual_ID\"></a>\n",
    "## Compare output RA and Dec to expected values\n",
    "\n",
    "The output RA and Dec coordinates should match the input RA and Dec coordinates to within 1/10 of a PSF FWHM (~0.03 arcsec for F770W).\n",
    "\n",
    "Output RA_Diff and Dec_diff above should be on order of 30 or fewer milliarcseconds.\n",
    "\n",
    "Check to see if your surface brightness is roughly what you expected based on the input data.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check flux values against spatial coordinates\n",
    "\n",
    "Another test that can be done is to plot the flux values against x and y values to check for any systmatic patterns. Previous testing has shown a spatial dependence of the flux with y values, such that there is a rise in surface brightness in the middle, and a drop at the image edges. A quick plot can show whether this problem is fixed or not. Prior to the resample step, there is no pattern, after the step, a pattern is clear. Just do this as a last check. If the scatter is not random, there may be a problem that needs to be checked. (Of course, this only works if you give an equivalent if not equal input count level to each input star.) If there is not an obvious rise in the middle (around y centroid pos of 550-600), then this test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Flux (MJy) vs. y position on detector')\n",
    "\n",
    "plt.ylim(1.05e-08,1.12e-08)\n",
    "plt.xlabel('y centroid position')\n",
    "plt.ylabel('Surface brightness')\n",
    "plt.plot(sources['ycentroid'], phot_table['scaled_ap_sum_bkgsub'], marker='o',linestyle='') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Flux (MJy) vs. x position on detector')\n",
    "\n",
    "plt.ylim(1.05e-08,1.12e-08)\n",
    "plt.xlabel('x centroid position')\n",
    "plt.ylabel('Surface brightness')\n",
    "plt.plot(sources['xcentroid'], phot_table['scaled_ap_sum_bkgsub'], marker='o',linestyle='') #ylim=(30000,40000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat prior tests with different scaling factor\n",
    "Check previous tests with different scaling factor to see if the RA/Dec values still line up and the photometry gives consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all *.cal files and create a new association table.\n",
    "callist = glob.glob('starfield*cal.fits')\n",
    "\n",
    "# use asn_from_list to create association table\n",
    "asn = asn_from_list.asn_from_list(callist, rule=DMS_Level3_Base, product_name='starfield_50star4ptdither_scaled_combined.fits')\n",
    "\n",
    "# use this if you need to add non'science' exposure types\n",
    "#asn['products'][0]['members'][1]['exptype'] = 'background'\n",
    "#asn['products'][0]['members'][2]['exptype'] = 'sourcecat'\n",
    "\n",
    "# dump association table to a .json file for use in image3\n",
    "with open('starfield_50star4ptdither_scaled_asnfile.json', 'w') as fp:\n",
    "    fp.write(asn.dump()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Calwebb_image3 on the association table\n",
    "    \n",
    "# set any specific parameters\n",
    "# tweakreg parameters to allow data to run\n",
    "fwhm = 5.0  # Gaussian kernel FWHM of objects expected, default=2.5\n",
    "minobj = 5  # minimum number of objects needed to match positions for a good fit, default=15\n",
    "snr = 50 # signal to noise threshold, default=5\n",
    "sigma = 3 # clipping limit, in sigma units, used when performing fit, default=3\n",
    "fit_geom = 'shift' # ftype of affine transformation to be considered when fitting catalogs, default='general'\n",
    "use2dhist = False  # boolean indicating whether to use 2D histogram to find initial offset, default=True\n",
    "\n",
    "ratio = 0.5 # Ratio of input to output pixel scale. A value of 0.5 means the output image would have 4 pixels \n",
    "            # sampling each input pixel.\n",
    "    \n",
    "pipe3=Image3Pipeline()    \n",
    "pipe3.tweakreg.kernel_fwhm = fwhm\n",
    "pipe3.tweakreg.snr_threshold = snr\n",
    "pipe3.tweakreg.minobj = minobj\n",
    "pipe3.tweakreg.sigma = sigma\n",
    "pipe3.tweakreg.fitgeometry = fit_geom\n",
    "#pipe3.tweakreg.skip = True\n",
    "pipe3.tweakreg.use2dhist = use2dhist\n",
    "pipe3.resample.pixel_scale_ratio = ratio\n",
    "pipe3.source_catalog.save_results = True\n",
    "pipe3.save_results = True\n",
    "\n",
    "# run Image3\n",
    "\n",
    "image = pipe3.run('starfield_50star4ptdither_scaled_asnfile.json')\n",
    "print('Image 3 pipeline finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the combined data file and list of coordinates\n",
    "\n",
    "im2 = ImageModel('starfield_50star4ptdither_scaled_combined_i2d.fits')\n",
    "#get pixel scale\n",
    "pixarea2 = im2.meta.photometry.pixelarea_steradians\n",
    "print('Pixel area in steradians', pixarea2)\n",
    "print('Ratio of pixel area scales: original/new scale', pixarea / pixarea2)\n",
    "\n",
    "# read in text file with RA and Dec input coordinates\n",
    "RA_in, Dec_in = np.loadtxt( coords, dtype=str, unpack=True)\n",
    "\n",
    "# put RA and Dec into floats\n",
    "RA_sim = RA_in.astype(float)\n",
    "Dec_sim = Dec_in.astype(float)\n",
    "\n",
    "\n",
    "# pull out data portion of input file\n",
    "data2 = im2.data\n",
    "\n",
    "# print stats on input image\n",
    "mean, median, std = sigma_clipped_stats(data2, sigma=200.0, maxiters=5)  # default sigma=3\n",
    "print('Image mean, median and std',mean, median, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DAOStarFinder to find sources in image\n",
    "pixel_ratio = math.sqrt(pixarea / pixarea2) # ratio of pixel area scales in single dimension (sqrt of area)\n",
    "\n",
    "ap_radius2 = ap_radius * pixel_ratio  # radius for aperture for centroiding and photometry\n",
    "print('New aperture radius', ap_radius2)\n",
    "\n",
    "daofind = DAOStarFinder(fwhm=3.0*pixel_ratio, threshold=10.*std)    # default threshold=5*std, fwhm=3\n",
    "sources2 = daofind(data2)    \n",
    "#print(sources2['xcentroid','ycentroid','peak'])   \n",
    "sources2.pprint_all()\n",
    "\n",
    "# Create apertures for x,y positions\n",
    "positions2 = tuple(zip(sources2['xcentroid'], sources2['ycentroid']))\n",
    "#print(positions)\n",
    "\n",
    "#positions = (sources['xcentroid'], sources['ycentroid'])\n",
    "apertures2 = CircularAperture(positions2, r=ap_radius2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark sources on image frame to see if the correct sources were found\n",
    "norm = ImageNormalize(stretch=SqrtStretch())\n",
    "# keep image stretch in mind for plotting. sky subtracted range ~ (-15, 10), single sample ~ (0, 20)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(data2, cmap='Greys', origin='lower', vmin=8,vmax=15)#, norm=norm)\n",
    "apertures2.plot(color='red', lw=2.5) #, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for inner and outer annuli to collect background counts\n",
    "\n",
    "inner_annulus2 = inner_annulus * pixel_ratio\n",
    "outer_annulus2 = outer_annulus * pixel_ratio\n",
    "\n",
    "# set up annulus for background\n",
    "background_aper2 = CircularAnnulus(positions2, r_in=inner_annulus2, r_out=outer_annulus2)\n",
    "\n",
    "# perform photometry on apertures for targets and background annuli\n",
    "phot_table2 = aperture_photometry(im2.data, apertures2)\n",
    "\n",
    "# perform background subtraction with outlier rejection\n",
    "bkg_median = []\n",
    "bkg_mask2 = background_aper2.to_mask(method='center')\n",
    "bmask2 = bkg_mask2[0]\n",
    "for mask in bkg_mask2:\n",
    "    aper_data = bmask2.multiply(data2)\n",
    "    aper_data = aper_data[mask.data > 0]\n",
    "    \n",
    "    # perform sigma-clipped median\n",
    "    _, median_sigclip, _ = sigma_clipped_stats(aper_data)\n",
    "    bkg_median.append(median_sigclip)\n",
    "bkg_median = np.array(bkg_median)\n",
    "\n",
    "\n",
    "# do calculations on background regions found in annuli\n",
    "# Get average background per pixel\n",
    "phot_table2['annulus_median'] = bkg_median\n",
    "# Get total background in the science aperture (per pixel * area in aperture)\n",
    "phot_table2['aperture_bkg'] = bkg_median * apertures2.area\n",
    "# subtract background in aperture from flux in aperture\n",
    "phot_table2['aperture_sum_bkgsub'] = phot_table2['aperture_sum'] - phot_table2['aperture_bkg']\n",
    "# put aperture sum in pixel scale\n",
    "phot_table2['scaled_ap_sum_bkgsub'] = phot_table2['aperture_sum_bkgsub'] * pixarea2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table2_sub = phot_table2['aperture_sum','annulus_median','aperture_bkg','aperture_sum_bkgsub','scaled_ap_sum_bkgsub']\n",
    "phot_table2_sub.pprint_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using wcs info from images, put coordinates into RA, Dec\n",
    "ra2, dec2 = im2.meta.wcs(sources2['xcentroid'], sources2['ycentroid'])\n",
    "\n",
    "# add RA, Dec to sources table\n",
    "\n",
    "ra_col = Column(name='RA', data=ra2)\n",
    "dec_col = Column(name='Dec', data=dec2)\n",
    "sources2.add_column(ra_col)\n",
    "sources2.add_column(dec_col)\n",
    "\n",
    "# print RA, Dec for each x, y position found\n",
    "#print(sources2['xcentroid', 'ycentroid', 'RA', 'Dec']) \n",
    "sources2_sub = sources2['xcentroid', 'ycentroid', 'RA', 'Dec']\n",
    "sources2_sub.pprint_all()\n",
    "\n",
    "# add option to print out list of sources with flux values\n",
    "outtable = 'sourcelist_phot_rate.txt'\n",
    "sources2.add_column(phot_table2['aperture_sum'])\n",
    "sources2.add_column(phot_table2['aperture_sum_bkgsub'])\n",
    "sources2.add_column(phot_table2['scaled_ap_sum_bkgsub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare input RA, Dec to found RA, Dec\n",
    "print('       RA found       Dec found    RA_Diff (mas)  Dec_diff (mas)  Bkg sub flux  pass/fail')\n",
    "\n",
    "deltara2 = []\n",
    "deltadec2 = []\n",
    "\n",
    "for i in np.arange(0,len(RA_sim)):\n",
    "    for j in np.arange(0,len(ra2)):\n",
    "        ra_diff = 180 - abs(abs(RA_sim[i] - ra2[j])-180)\n",
    "        dec_diff = 180 - abs(abs(Dec_sim[i] - dec2[j])-180)\n",
    "\n",
    "        if ra_diff < 1e-5 and dec_diff < 1e-5:\n",
    "            # put differences in milliarcseconds\n",
    "            ra_diff = ra_diff * 3600000\n",
    "            dec_diff = dec_diff * 3600000\n",
    "            deltara2.append(ra_diff)\n",
    "            deltadec2.append(dec_diff)\n",
    "            if ra_diff < 30 and dec_diff < 30: \n",
    "                test = 'pass' \n",
    "            else: \n",
    "                test = 'fail'\n",
    "            print('{:15.6f} {:15.6f} {:15.6f} {:15.6f} {:15.6e} {}'.format(ra2[j], dec2[j], ra_diff, dec_diff, \n",
    "                                                                        phot_table2['scaled_ap_sum_bkgsub'][j], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ra and dec differences\n",
    "plt.title ('Differences in RA and Dec in millarcseconds')\n",
    "plt.ylabel('Delta RA')\n",
    "plt.xlabel('Delta Dec')\n",
    "plt.scatter(deltadec2,deltara2)\n",
    "plt.show()\n",
    "\n",
    "# Plot should show no differences greater than 30 milliarcseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare photometry between the original and scaled images\n",
    "plt.title('Flux (MJy) vs. y position on detector')\n",
    "#plt.ylim(33500,39000) # help weed out sources that were erroneously 'hits' (bad pixels, cosmic rays, etc)\n",
    "plt.ylim(1.05e-08,1.12e-08)\n",
    "plt.xlabel('y centroid position')\n",
    "plt.ylabel('Surface brightness')\n",
    "plt.plot(sources['ycentroid'], phot_table['scaled_ap_sum_bkgsub'], marker='o',linestyle='') \n",
    "plt.plot(sources2['ycentroid']/2, phot_table2['scaled_ap_sum_bkgsub'], marker='+',linestyle='') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare photometry between the original and scaled images\n",
    "plt.title('Flux (MJy) vs. x position on detector')\n",
    "#plt.ylim(33500,39000) # help weed out sources that were erroneously 'hits' (bad pixels, cosmic rays, etc)\n",
    "plt.ylim(1.05e-08,1.12e-08)\n",
    "plt.xlabel('x centroid position')\n",
    "plt.ylabel('Surface brightness')\n",
    "plt.plot(sources['xcentroid'], phot_table['scaled_ap_sum_bkgsub'], marker='o',linestyle='') \n",
    "plt.plot(sources2['xcentroid']/2, phot_table2['scaled_ap_sum_bkgsub'], marker='+',linestyle='') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some statistics of the two sets of fluxes for comparison\n",
    "# get stddev from mean for each of the measurements in percentage\n",
    "\n",
    "photdata1 = phot_table['scaled_ap_sum_bkgsub']\n",
    "photdata2 = phot_table2['scaled_ap_sum_bkgsub']\n",
    "\n",
    "std1 = np.std(photdata1)\n",
    "mean1 = np.mean(photdata1)\n",
    "\n",
    "std2 = np.std(photdata2)\n",
    "mean2 = np.mean(photdata2)\n",
    "\n",
    "per1 = std1 / mean1 * 100\n",
    "per2 = std2 / mean2 * 100\n",
    "\n",
    "print('Mean and standard deviation of original scale data', mean1, std1)\n",
    "print('Mean and standard deviation of new scale data', mean2, std2)\n",
    "\n",
    "print('The standard deviation from the mean (as a percentage) for the original scale flux data, is', per1)\n",
    "print('The standard deviation from the mean (as a percentage) for the new scaled flux data, is', per2)\n",
    "\n",
    "# Compare stats on overall image values, not just phot values\n",
    "\n",
    "# im is the original scaled image and im2 is the rescaled image\n",
    "\n",
    "# print stats on original input image\n",
    "mean1, median1, std1 = sigma_clipped_stats(data, sigma=200.0, maxiters=5)  # default sigma=3\n",
    "print('Original image mean, median and std',mean1, median1, std1)\n",
    "\n",
    "# print stats on rescaled input image\n",
    "mean2, median2, std2 = sigma_clipped_stats(data2, sigma=200.0, maxiters=5)  # default sigma=3\n",
    "print('Rescaled image mean, median and std',mean2, median2, std2)\n",
    "\n",
    "ratio_mean = mean1 / mean2\n",
    "#ratio_median = median1 / median2\n",
    "\n",
    "print('The ratio of the mean of the two images is: ', ratio_mean)\n",
    "#print('The ratio of the median of the two images is: ', ratio_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing criteria\n",
    "If all RA/Dec matches have a 'pass' in the table for both original and scaled images, the fluxes for both the original data set and the scaled data set are roughly equal and the mean fluxes are nearly equivalent, then the notebook passes. The ratio printed of the means of the two images should be approximately 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about_ID\"></a>\n",
    "## About this Notebook\n",
    "**Author:** M. Cracraft, Senior Staff Scientist, INS/MIRI\n",
    "<br>**Updated On:** 04/02/2021 to add in testing the 'ratio' scaling option in resample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#title_ID)\n",
    "<img style=\"float: right;\" src=\"./stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"stsci_pri_combo_mark_horizonal_white_bkgd\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
