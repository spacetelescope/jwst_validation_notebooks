{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Testing Notebook: MIRI LRS Slit\n",
    "\n",
    "## Spec2: Extract1d()\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: MIRI\n",
    "\n",
    "### Table of Contents\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "<br> [Imports](#imports_ID) <br> [Introduction](#intro_ID) <br> [Get Documentaion String for Markdown Blocks](#markdown_from_docs) <br> [Loading Data](#data_ID) <br> [Run JWST Pipeline](#pipeline_ID) <br> [Create Figure or Print Output](#residual_ID) <br> [About This Notebook](#about_ID) <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports_ID\"></a>\n",
    "# Imports\n",
    "List the library imports and why they are relevant to this notebook.\n",
    "\n",
    "* os, glob for general  OS operations\n",
    "* numpy\n",
    "* astropy.io for opening fits files\n",
    "* inspect to get the docstring of our objects.\n",
    "* IPython.display for printing markdown output\n",
    "* jwst.datamodels for building model for JWST Pipeline\n",
    "* jwst.module.PipelineStep is the pipeline step being tested\n",
    "* matplotlib.pyplot to generate plot\n",
    "* json for editing json files\n",
    "* crds for retrieving reference files as needed\n",
    "\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "import astropy.units as u\n",
    "import jwst.datamodels as datamodels\n",
    "from jwst.datamodels import RampModel, ImageModel\n",
    "from jwst.pipeline import Detector1Pipeline, Spec2Pipeline\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "from gwcs.wcstools import grid_from_bounding_box\n",
    "from jwst.associations.asn_from_list import asn_from_list\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase\n",
    "\n",
    "\n",
    "import json\n",
    "import crds\n",
    "\n",
    "from ci_watson.artifactory_helpers import get_bigdata\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro_ID\"></a>\n",
    "# Introduction\n",
    "\n",
    "\n",
    "In this notebook we will test the **extract1d()** step of Spec2Pipeline() for **LRS slit** observations.\n",
    "\n",
    "Step description: https://jwst-pipeline.readthedocs.io/en/stable/jwst/extract_1d/index.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst/extract_1d\n",
    "\n",
    "### Short description of the algorithm\n",
    "\n",
    "The extract1d() step does the following for POINT source observations:\n",
    "\n",
    "* the code searches for the ROW that is the centre of the bounding box y-range\n",
    "* using the WCS information attached to the data in assign_wcs() it will determine the COLUMN location of the target\n",
    "* it computes the difference between this location and the centre of the bounding box x-range\n",
    "* BY DEFAULT the exraction aperture will be centred on the target location, and the flux in the aperture will be summed row by row. The default extraction width is a constant value of 11 PIXELS.\n",
    "\n",
    "In **Test 1** below, we load the json file that contains the default parameters, and override these to perform extraction over the full bounding box width for a single exposure. This tests the basic arithmetic of the extraction.\n",
    "\n",
    "In **Test 2**, we use the pair of nodded exposures and pass these in an association to the Spec2Pipeline, allowing extract1d() to perform the default extraction on a nodded pair. This is a very typical LRS science use case.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_ID\"></a>\n",
    "# Loading Data\n",
    "\n",
    "We are using here a simulated LRS slit observation, generated with MIRISim v2.3.0 (as of Dec 2020). It is a simple along-slit-nodded observation of a point source (the input was modelled on the flux calibrator BD+60). LRS slit observations cover the full array. \n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slitfile1 = get_bigdata('jwst_validation_notebooks',\n",
    "                                     'validation_data',\n",
    "                                     'calwebb_spec2',\n",
    "                                    'spec2_miri_test',\n",
    "                                    'miri_lrs_slit_pt_nod1_v2.3.fits')\n",
    " \n",
    "Slitfile2 = get_bigdata('jwst_validation_notebooks',\n",
    "                                     'validation_data',\n",
    "                                     'calwebb_spec2',\n",
    "                                    'spec2_miri_test',\n",
    "                                    'miri_lrs_slit_pt_nod2_v2.3.fits')\n",
    "\n",
    "files = [Slitfile1, Slitfile2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect configuration files locally if they aren't yet there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../cfg_files/'):\n",
    "    os.mkdir('../cfg_files/')\n",
    "    cfgs = collect_pipeline_cfgs.collect_pipeline_cfgs(dst='../cfg_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline_ID\"></a>\n",
    "# Run JWST Pipeline\n",
    "\n",
    "First we run the data through the Detector1() pipeline to convert the raw counts into slopes. This should use the calwebb_detector1.cfg file. The output of this stage will then be run through the Spec2Pipeline. Extract_1d is the final step of this pipeline stage, so we will just run through the whole pipeline.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector1Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det1 = []\n",
    "\n",
    "# Run pipeline on both files\n",
    "for ff in files:\n",
    "    d1 = Detector1Pipeline.call(ff, save_results=True, config_file='../cfg_files/calwebb_detector1.cfg')\n",
    "    det1.append(d1)\n",
    "\n",
    "print(det1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spec2Pipeline\n",
    "\n",
    "Next we go ahead to the Spec2 pipeline. At this stage we perform 2 tests:\n",
    "\n",
    "1. run the Spec2 pipeline on one single exposure, extracting over the full bounding box width. we compare this with the manual extraction over the same aperture. this tests whether the pipeline is performing the correct arithmetic in the extraction procedure.\n",
    "2. run the Spec2 pipeline on the nodded set of exposures. this mimics more closely how the pipeline will be run in automated way during routine operations. this will test whether the pipeline is finding the source positions, and is able to extract both nodded observations in the same way.\n",
    "\n",
    "The initial steps will be the same for both tests and will be run on both initially.\n",
    "\n",
    "First we run the Spec2Pipeline() **skipping** the extract1d() step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec2 = []\n",
    "for dd in det1:\n",
    "    s2 = Spec2Pipeline.call(dd.meta.filename, config_file='../cfg_files/calwebb_spec2.cfg', steps={\"extract_1d\": {\"skip\": True}})\n",
    "    spec2.append(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calfiles = glob('*_cal.fits')\n",
    "print(calfiles)\n",
    "photom = []\n",
    "nods = []\n",
    "\n",
    "for cf in calfiles:\n",
    "    if 'nod1' in cf:\n",
    "        nn = 'nod1'\n",
    "    else:\n",
    "        nn = 'nod2'\n",
    "    ph = datamodels.open(cf)\n",
    "    photom.append(ph)\n",
    "    nods.append(nn)\n",
    "    \n",
    "print(photom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the wcs information from the PHOTOM output file so we know the coordinates of the bounding box and the wavelength grid. We use the ``grid_from_bounding_box`` function to generate these grids. We convert the wavelength grid into a wavelength vector by averaging over each row. This works because LRS distortion is minimal, so lines of equal wavelength run along rows (not 100% accurate but for this purpose this is correct).\n",
    "\n",
    "This cell performs a check that both nods have the same wavelength assignment over the full bounding box, which is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = []\n",
    "\n",
    "for ph,nn in zip(photom, nods):\n",
    "\n",
    "    bbox_w = ph.meta.wcs.bounding_box[0][1] - ph.meta.wcs.bounding_box[0][0]\n",
    "    bbox_ht = ph.meta.wcs.bounding_box[1][1] - ph.meta.wcs.bounding_box[1][0]\n",
    "    print('Model bbox ({1}) = {0} '.format(ph.meta.wcs.bounding_box, nn))\n",
    "    print('Model: Height x width of bounding box ({2})= {0} x {1} pixels'.format(bbox_ht, bbox_w, nn))\n",
    "\n",
    "    x,y = grid_from_bounding_box(ph.meta.wcs.bounding_box)\n",
    "    ra, dec, lam = ph.meta.wcs(x, y)\n",
    "\n",
    "    lam_vec = np.mean(lam, axis=1)\n",
    "    lams.append(lam_vec)\n",
    "    \n",
    "# check that the wavelength vectors for the nods are equal, then we can just work with one going forward\n",
    "assert np.array_equal(lams[0], lams[1], equal_nan=True), \"Arrays not equal!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Single exposure, full width extraction\n",
    "\n",
    "To enable the extraction over the full width of the LRS slit bounding box, we have to edit the json parameters file and run the step with an override to the config file. We first run the Spec2Pipeline with its default settings, skipping the extract_1d() step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next few steps will be executed with one of the nods only.** Next we perform a manual extraction by first extracting the bounding box portion of the array, and then summing up the values in each row over the full BB width. This returns the flux in MJy/sr, which we convert to Jy using the pixel area. A MIRI imager pixel measures 0.11\" on the side.\n",
    "\n",
    "**NOTE: the current pipeline step applies a nod offset of -8.284xxx pix or +8.805xxx pix for nod1, nod 2 respectively, regardless of what's in the json file. I have added this offset to this test to demonstrate that we understand and are able to macth the pipeline very closely, but this will very likely need updating as it is not realistic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph1 = photom[0]\n",
    "nn = nods[0]\n",
    "print('The next steps will be run only on {0}, the {1} exposure'.format(ph1.meta.filename, nn))\n",
    "\n",
    "if (nn == 'nod1'):\n",
    "    offset = -8.28479\n",
    "else:\n",
    "    offset = 8.80479\n",
    "\n",
    "photom_sub = ph1.data[np.int(np.min(y)):np.int(np.max(y)+1), np.int(np.min(x)+offset):np.int(np.max(x)+1+offset)]\n",
    "print('Cutout has dimensions ({0})'.format(np.shape(photom_sub)))\n",
    "print('The cutout was taken from pixel {0} to pixel {1} in x'.format(np.int(np.min(x)+offset),np.int(np.max(x)+1+offset)))\n",
    "\n",
    "xsub = np.sum(photom_sub, axis=1)\n",
    "\n",
    "#remove some nans\n",
    "lam_vec = lams[0]\n",
    "xsub = xsub[~np.isnan(lam_vec)]\n",
    "lam_vec = lam_vec[~np.isnan(lam_vec)]\n",
    "\n",
    "# calculate the pixel area in sr\n",
    "pix_scale = 0.11 * u.arcsec\n",
    "pixar_as2 = pix_scale**2\n",
    "pixar_sr = pixar_as2.to(u.sr)\n",
    "\n",
    "# now convert flux from MJy/sr to Jy using the pixel area\n",
    "if (ph1.meta.bunit_data == 'MJy/sr'):\n",
    "    xsub_cal = xsub * pixar_sr.value * 1e6\n",
    "\n",
    "print(len(lam_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the ``extract_1d()`` step on the same file, editing the configuration to sum up over the entire aperture as we did above. We load in the json file, make ajustments and run the step with a config file override option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreffile='jwst_miri_extract1d_0004.json'\n",
    "basename=crds.core.config.pop_crds_uri(extreffile)\n",
    "path=crds.locate_file(basename,\"jwst\")\n",
    "with open(path) as json_ref:\n",
    "    jsreforig = json.load(json_ref)\n",
    "    jsrefdict = jsreforig.copy()\n",
    "    jsrefdict['apertures'][0]['xstart'] = np.int(np.min(x))\n",
    "    jsrefdict['apertures'][0]['xstop'] = np.int(np.max(x)) + 1\n",
    "    \n",
    "    for element in jsrefdict['apertures']:\n",
    "        element.pop('extract_width', None)\n",
    "        element.pop('nod2_offset', None)\n",
    "\n",
    "with open('extract1d_fs2b75.json','w') as jsrefout:\n",
    "    json.dump(jsrefdict,jsrefout,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extract_1d_fs2b75.cfg','w') as cfg:\n",
    "    cfg.write('name = \"extract_1d\"'+'\\n')\n",
    "    cfg.write('class = \"jwst.extract_1d.Extract1dStep\"'+'\\n')\n",
    "    cfg.write(''+'\\n')\n",
    "    cfg.write('log_increment = 50'+'\\n')\n",
    "    cfg.write('smoothing_length = 0'+'\\n')\n",
    "    cfg.write('override_extract1d=\"extract1d_fs2b75.json\"'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsub_pipe = Extract1dStep.call(ph1, config_file='extract_1d_fs2b75.cfg', save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the step ran successfully, we can now look at the output and compare to our manual extraction spectrum. To ratio the 2 spectra we interpolate the manually extracted spectrum ``xsub_cal`` onto the pipeline-generated wavelength grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=[14,5])\n",
    "ax[0].imshow(photom_sub, origin='lower', interpolation='None')\n",
    "ax[1].plot(lam_vec, xsub_cal, 'b-', label='manual extraction')\n",
    "ax[1].plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], xsub_pipe.spec[0].spec_table['FLUX'], 'g-', label='pipeline extraction')\n",
    "ax[1].set_title('{0}, Full BBox extraction'.format(nn))\n",
    "\n",
    "#interpolate the two onto the same grid so we can look at the difference\n",
    "f = interp1d(lam_vec, xsub_cal, fill_value='extrapolate')\n",
    "ixsub_cal = f(xsub_pipe.spec[0].spec_table['WAVELENGTH'])\n",
    "\n",
    "diff = ((xsub_pipe.spec[0].spec_table['FLUX'] - ixsub_cal) / xsub_pipe.spec[0].spec_table['FLUX']) * 100.\n",
    "\n",
    "ax[2].plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], diff, 'k-')\n",
    "ax[2].set_title('Difference manual - pipeline extracted spectra')\n",
    "ax[2].set_ylim([-20., 20.])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the ratio between the 2 is on average <= 2 per cent in the core range between 5 and 10 micron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = (xsub_pipe.spec[0].spec_table['WAVELENGTH'] >= 5.0) & (xsub_pipe.spec[0].spec_table['WAVELENGTH'] <= 10.)\n",
    "print(np.mean(diff[inds]))\n",
    "assert np.mean(diff[inds]) <= 2.0, \"Mean difference between pipeline and manual extraction >= 1 per cent in 5-10 um. CHECK.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------**END OF TEST PART 1**----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Nodded observation, two exposures\n",
    "\n",
    "In this second test we use both nodded observations. In this scenarion, the nods are used as each other's background observations and we need to ensure that the extraction aperture is placed in the right position with a realistic aperture for both nods.\n",
    "\n",
    "We will re-run the first steps of the Spec2Pipeline, so that the nods are used as each other's backgrounds. This requires creation of an association from which the Spec2Pipeline will be run. Then we will run them both through the extract_1d() step with the default parameters, checking:\n",
    "* the location of the aperture\n",
    "* the extraction width\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_files = [det1[0].meta.filename, det1[1].meta.filename]\n",
    "asn = asn_from_list(asn_files, rule=DMSLevel2bBase, meta={'program':'test', 'target':'bd60', 'asn_pool':'test'})\n",
    "\n",
    "# now add the opposite nod as background exposure:\n",
    "asn['products'][0]['members'].append({'expname': 'miri_lrs_slit_pt_nod2_v2.3_rate.fits', 'exptype':'background'})\n",
    "asn['products'][1]['members'].append({'expname': 'miri_lrs_slit_pt_nod1_v2.3_rate.fits', 'exptype':'background'})\n",
    "\n",
    "# write this out to a json file\n",
    "with open('lrs-slit-test_asn.json', 'w') as fp:\n",
    "    fp.write(asn.dump()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the Spec2Pipeline with this association files as input, instead of the individual FITS files or datamodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = Spec2Pipeline.call('lrs-slit-test_asn.json', save_results=True, config_file='../cfg_files/calwebb_spec2.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1dfiles = glob('*_x1d.fits')\n",
    "print(x1dfiles)\n",
    "x1ds = [datamodels.open(xf) for xf in x1dfiles]\n",
    "fig = plt.figure(figsize=[15,5])\n",
    "\n",
    "for x1 in x1ds:\n",
    "    if 'nod1' in x1.meta.filename:\n",
    "        nn = 'nod1'\n",
    "    else: \n",
    "        nn = 'nod2'\n",
    "    plt.plot(x1.spec[0].spec_table['WAVELENGTH'], x1.spec[0].spec_table['FLUX'], label=nn)\n",
    "\n",
    "plt.plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], xsub_pipe.spec[0].spec_table['FLUX'], label='nod 1, full bbox extracted')\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()\n",
    "plt.xlim([4.5, 12.5])\n",
    "plt.xlabel('wavelength (micron)', fontsize='large')\n",
    "plt.ylabel('Jy', fontsize='large')\n",
    "fig.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will test for:\n",
    "* the extracted spectra should be near-identical (chosen criteria: mean ration between the 2 <= 5%)\n",
    "\n",
    "\n",
    "**Further tests to add:**\n",
    "* perform a full verification of the extraction at the source position and with the same extraction width as in the parameters file.\n",
    "\n",
    "**If the ``assert`` statement below passes, we consider the test a success.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = x1ds[0].spec[0].spec_table['FLUX'] > 0.00\n",
    "ratio = x1ds[0].spec[0].spec_table['FLUX'][inds] / x1ds[1].spec[0].spec_table['FLUX'][inds]\n",
    "infs = np.isinf(ratio-1.0)\n",
    "assert np.mean(np.abs(ratio[~infs] - 1.)) <= 0.05, \"Extracted spectra don't match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Authors: B. Sargent/S. Kendrew, MIRI branch\n",
    "* Last updated: Jan 11th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
