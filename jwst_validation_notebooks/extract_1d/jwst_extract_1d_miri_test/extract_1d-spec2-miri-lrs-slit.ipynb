{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# JWST Pipeline Validation Testing Notebook: MIRI LRS Slit\n",
    "\n",
    "## Spec2: Extract1d()\n",
    "<span style=\"color:red\"> **Instruments Affected**</span>: MIRI\n",
    "\n",
    "### Table of Contents\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "<br> [Imports](#imports_ID) <br> [Introduction](#intro_ID) <br> [Get Documentaion String for Markdown Blocks](#markdown_from_docs) <br> [Loading Data](#data_ID) <br> [Run JWST Pipeline](#pipeline_ID) <br> [Create Figure or Print Output](#residual_ID) <br> [About This Notebook](#about_ID) <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports_ID\"></a>\n",
    "# Imports\n",
    "List the library imports and why they are relevant to this notebook.\n",
    "\n",
    "* os, glob for general  OS operations\n",
    "* numpy\n",
    "* astropy.io for opening fits files\n",
    "* inspect to get the docstring of our objects.\n",
    "* IPython.display for printing markdown output\n",
    "* jwst.datamodels for building model for JWST Pipeline\n",
    "* jwst.module.PipelineStep is the pipeline step being tested\n",
    "* matplotlib.pyplot to generate plot\n",
    "* json for editing json files\n",
    "* crds for retrieving reference files as needed\n",
    "\n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to hold notebook output, and change the working directory to that directory.\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "data_dir = TemporaryDirectory()\n",
    "os.chdir(data_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "import astropy.units as u\n",
    "import jwst.datamodels as datamodels\n",
    "from jwst.datamodels import RampModel, ImageModel\n",
    "from jwst.pipeline import Detector1Pipeline, Spec2Pipeline\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "from gwcs.wcstools import grid_from_bounding_box\n",
    "from jwst.associations.asn_from_list import asn_from_list\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase\n",
    "\n",
    "\n",
    "import json\n",
    "import crds\n",
    "\n",
    "from ci_watson.artifactory_helpers import get_bigdata\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro_ID\"></a>\n",
    "# Introduction\n",
    "\n",
    "\n",
    "In this notebook we will test the **extract1d()** step of Spec2Pipeline() for **LRS slit** observations.\n",
    "\n",
    "Step description: https://jwst-pipeline.readthedocs.io/en/stable/jwst/extract_1d/index.html\n",
    "\n",
    "Pipeline code: https://github.com/spacetelescope/jwst/tree/master/jwst/extract_1d\n",
    "\n",
    "### Short description of the algorithm\n",
    "\n",
    "The extract1d() step does the following for POINT source observations:\n",
    "\n",
    "* the code searches for the ROW that is the centre of the bounding box y-range\n",
    "* using the WCS information attached to the data in assign_wcs() it will determine the COLUMN location of the target\n",
    "* it computes the difference between this location and the centre of the bounding box x-range\n",
    "* BY DEFAULT the exraction aperture will be centred on the target location, and the flux in the aperture will be summed row by row. The default extraction width is a constant value of 11 PIXELS.\n",
    "\n",
    "In **Test 1** below, we load the json file that contains the default parameters, and override these to perform extraction over the full bounding box width for a single exposure. This tests the basic arithmetic of the extraction.\n",
    "\n",
    "In **Test 2**, we use the pair of nodded exposures and pass these in an association to the Spec2Pipeline, allowing extract1d() to perform the default extraction on a nodded pair. This is a very typical LRS science use case.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_ID\"></a>\n",
    "# Loading Data\n",
    "\n",
    "We are using here a simulated LRS slit observation, generated with MIRISim v2.3.0 (as of Dec 2020). It is a simple along-slit-nodded observation of a point source (the input was modelled on the flux calibrator BD+60). LRS slit observations cover the full array. \n",
    "\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slitfile1 = get_bigdata('jwst_validation_notebooks',\n",
    "                                     'validation_data',\n",
    "                                     'calwebb_spec2',\n",
    "                                    'spec2_miri_test',\n",
    "                                    'miri_lrs_slit_pt_nod1_v2.3.fits')\n",
    " \n",
    "Slitfile2 = get_bigdata('jwst_validation_notebooks',\n",
    "                                     'validation_data',\n",
    "                                     'calwebb_spec2',\n",
    "                                    'spec2_miri_test',\n",
    "                                    'miri_lrs_slit_pt_nod2_v2.3.fits')\n",
    "\n",
    "files = [Slitfile1, Slitfile2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline_ID\"></a>\n",
    "# Run JWST Pipeline\n",
    "\n",
    "First we run the data through the Detector1() pipeline to convert the raw counts into slopes. This should use the calwebb_detector1.cfg file. The output of this stage will then be run through the Spec2Pipeline. Extract_1d is the final step of this pipeline stage, so we will just run through the whole pipeline.\n",
    "\n",
    "[Top of Page](#title_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector1Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det1 = []\n",
    "\n",
    "# Run pipeline on both files\n",
    "for ff in files:\n",
    "    d1 = Detector1Pipeline.call(ff, save_results=True)\n",
    "    det1.append(d1)\n",
    "\n",
    "print(det1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spec2Pipeline\n",
    "\n",
    "Next we go ahead to the Spec2 pipeline. At this stage we perform 2 tests:\n",
    "\n",
    "1. run the Spec2 pipeline on one single exposure, extracting over the full bounding box width. we compare this with the manual extraction over the same aperture. this tests whether the pipeline is performing the correct arithmetic in the extraction procedure.\n",
    "2. run the Spec2 pipeline on the nodded set of exposures. this mimics more closely how the pipeline will be run in automated way during routine operations. this will test whether the pipeline is finding the source positions, and is able to extract both nodded observations in the same way.\n",
    "\n",
    "The initial steps will be the same for both tests and will be run on both initially. Spectral extraction is performed on the output file of the 2D resampled images (\\_s2d.fits)\n",
    "\n",
    "First we run the Spec2Pipeline() **skipping** the extract1d() step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec2 = []\n",
    "for dd in det1:\n",
    "    s2 = Spec2Pipeline.call(dd.meta.filename,save_results=True, steps={\"extract_1d\": {\"skip\": True}})\n",
    "    spec2.append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calfiles = glob('*_cal.fits')\n",
    "calfiles = glob('*_s2d.fits')\n",
    "print(calfiles)\n",
    "s2d = []\n",
    "nods = []\n",
    "\n",
    "for cf in calfiles:\n",
    "    if 'nod1' in cf:\n",
    "        nn = 'nod1'\n",
    "    else:\n",
    "        nn = 'nod2'\n",
    "    ph = datamodels.open(cf)\n",
    "    s2d.append(ph)\n",
    "    nods.append(nn)\n",
    "    \n",
    "print(s2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the wcs information from the S2D output file so we know the coordinates of the bounding box and the wavelength grid. We use the ``grid_from_bounding_box`` function to generate these grids. We convert the wavelength grid into a wavelength vector by averaging over each row. This works because LRS distortion is minimal, so lines of equal wavelength run along rows (not 100% accurate but for this purpose this is correct).\n",
    "\n",
    "This cell performs a check that both nods have the same wavelength assignment over the full bounding box, which is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = []\n",
    "\n",
    "for ss,nn in zip(s2d, nods):\n",
    "\n",
    "    bbox_w = ss.meta.wcs.bounding_box[0][1] - ss.meta.wcs.bounding_box[0][0]\n",
    "    bbox_ht = ss.meta.wcs.bounding_box[1][1] - ss.meta.wcs.bounding_box[1][0]\n",
    "    print('Model bbox ({1}) = {0} '.format(ss.meta.wcs.bounding_box, nn))\n",
    "    print('Model: Height x width of bounding box ({2})= {0} x {1} pixels'.format(bbox_ht, bbox_w, nn))\n",
    "\n",
    "    x,y = grid_from_bounding_box(ss.meta.wcs.bounding_box)\n",
    "    ra, dec, lam = ss.meta.wcs(x, y)\n",
    "\n",
    "    lam_vec = np.mean(lam, axis=1)\n",
    "    lams.append(lam_vec)\n",
    "    \n",
    "# check that the wavelength vectors for the nods are equal, then we can just work with one going forward\n",
    "assert np.array_equal(lams[0], lams[1], equal_nan=True), \"Arrays not equal!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Single exposure, full width extraction\n",
    "\n",
    "To enable the extraction over the full width of the LRS slit bounding box, we have to edit the json parameters file and run the step with an override to the config file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next few steps will be executed with one of the nods only.** Next we perform a manual extraction by first extracting the bounding box portion of the array, and then summing up the values in each row over the full BB width. This returns the flux in MJy/sr, which we convert to Jy using the pixel area. A MIRI imager pixel measures 0.11\" on the side.\n",
    "\n",
    "**NOTE: as per default, the extract_1d() pipeline step will find the location of the target and offset the extraction window to be centred on the target. To extract the full slit, we want this to be disabled, so we set use_source_posn to False in the cfg input file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = s2d[0]\n",
    "nn = nods[0]\n",
    "print('The next steps will be run only on {0}, the {1} exposure'.format(s1.meta.filename, nn))\n",
    "\n",
    "#photom_sub = ph1.data[int(np.min(y)):int(np.max(y)+1), int(np.min(x)):int(np.max(x)+1)]\n",
    "s2d_sub = s2d[0].data\n",
    "print('Cutout has dimensions ({0})'.format(np.shape(s2d_sub)))\n",
    "print('The cutout was taken from pixel {0} to pixel {1} in x'.format(int(np.min(x)),int(np.max(x)+1)))\n",
    "\n",
    "xsub = np.sum(s2d_sub, axis=1)\n",
    "\n",
    "#remove some nans\n",
    "lam_vec = lams[0]\n",
    "xsub = xsub[~np.isnan(lam_vec)]\n",
    "lam_vec = lam_vec[~np.isnan(lam_vec)]\n",
    "\n",
    "# calculate the pixel area in sr\n",
    "pix_scale = 0.11 * u.arcsec\n",
    "pixar_as2 = pix_scale**2\n",
    "pixar_sr = pixar_as2.to(u.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to apply the aperture correction (new from B7.6). We load in the aperture correction reference file, identify the correct values, and multiply the calibrated spectrum by the correct numbers. When we are extracting over the full aperture, the correction should effectively be 1 as we are not losing any of the flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcorr_file = 'jwst_miri_apcorr_0007.fits'\n",
    "\n",
    "# retrieve this file and open as datamodel\n",
    "basename = crds.core.config.pop_crds_uri(apcorr_file)\n",
    "filepath = crds.locate_file(basename, \"jwst\")\n",
    "acref =  datamodels.open(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that list item 0 is for the slit mode (subarray = FULL)\n",
    "ind = 0\n",
    "assert acref.apcorr_table[ind]['subarray']=='FULL', \"index does not correspond to the correct subarray!\"\n",
    "\n",
    "xwidth = int(np.max(x))+1 - int(np.min(x))\n",
    "print(\"extraction width = {0}\".format(xwidth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first identify where the aperture width is in the \"size\" array\n",
    "if xwidth >= np.max(acref.apcorr_table[ind]['size']):\n",
    "    size_ind = np.argwhere(acref.apcorr_table[ind]['size'] == np.max(acref.apcorr_table[ind]['size']))\n",
    "else:\n",
    "    size_ind = np.argwhere(acref.apcorr_table[ind]['size'] == xwidth)\n",
    "\n",
    "# take the vector from the apcorr_table at this location and extract. \n",
    "apcorr_vec = acref.apcorr_table[ind]['apcorr'][:,size_ind[0][0]]\n",
    "print(np.shape(apcorr_vec))\n",
    "print(np.shape(acref.apcorr_table[ind]['wavelength']))\n",
    "\n",
    "# now we create an interpolated vector of values corresponding to the lam_vec wavelengths. \n",
    "# NOTE: the wavelengths are running in descending order so make sure assume_sorted = FALSE\n",
    "intp_ac = interp1d(acref.apcorr_table[ind]['wavelength'], apcorr_vec, kind='linear', assume_sorted=False)\n",
    "iapcorr = intp_ac(lam_vec)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.plot(acref.apcorr_table[1]['wavelength'], apcorr_vec, 'g-', label='ref file')\n",
    "plt.plot(lam_vec, iapcorr, 'r-', label='interpolated')\n",
    "#plt.plot(lam_vec, ac_vals, 'r-', label='aperture corrections for {} px ap'.format(xwidth))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now multiply the manually extracted spectra by the flux scaling and aperture correction vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert flux from MJy/sr to Jy using the pixel area\n",
    "if (s1.meta.bunit_data == 'MJy/sr'):\n",
    "    xsub_cal = xsub * pixar_sr.value * 1e6 * iapcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the ``extract_1d()`` step on the same file, editing the configuration to sum up over the entire aperture as we did above. We load in the json file, make adjustments and run the step with a config file override option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreffile='jwst_miri_extract1d_0004.json'\n",
    "basename=crds.core.config.pop_crds_uri(extreffile)\n",
    "path=crds.locate_file(basename,\"jwst\")\n",
    "with open(path) as json_ref:\n",
    "    jsreforig = json.load(json_ref)\n",
    "    jsrefdict = jsreforig.copy()\n",
    "    jsrefdict['apertures'][0]['xstart'] = int(np.min(x))\n",
    "    jsrefdict['apertures'][0]['xstop'] = int(np.max(x)) + 1\n",
    "    #jsrefdict['apertures'][0]['use_source_posn'] = False\n",
    "    \n",
    "    for element in jsrefdict['apertures']:\n",
    "        element.pop('extract_width', None)\n",
    "        element.pop('nod2_offset', None)\n",
    "\n",
    "with open('extract1d_slit_full_spec2.json','w') as jsrefout:\n",
    "    json.dump(jsrefdict,jsrefout,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extract1d_slit_full_spec2.cfg','w') as cfg:\n",
    "    cfg.write('name = \"extract_1d\"'+'\\n')\n",
    "    cfg.write('class = \"jwst.extract_1d.Extract1dStep\"'+'\\n')\n",
    "    cfg.write(''+'\\n')\n",
    "    cfg.write('log_increment = 50'+'\\n')\n",
    "    cfg.write('smoothing_length = 0'+'\\n')\n",
    "    cfg.write('use_source_posn = False' + '\\n')\n",
    "    cfg.write('override_extract1d=\"extract1d_slit_full_spec2.json\"'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsub_pipe = Extract1dStep.call(s1, config_file='extract1d_slit_full_spec2.cfg', save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the step ran successfully, we can now look at the output and compare to our manual extraction spectrum. To ratio the 2 spectra we interpolate the manually extracted spectrum ``xsub_cal`` onto the pipeline-generated wavelength grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=[14,5])\n",
    "ax[0].imshow(s2d_sub, origin='lower', interpolation='None')\n",
    "ax[1].plot(lam_vec, xsub_cal, 'b-', label='manual extraction')\n",
    "ax[1].plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], xsub_pipe.spec[0].spec_table['FLUX'], 'g-', label='pipeline extraction')\n",
    "ax[1].set_title('{0}, Full BBox extraction'.format(nn))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlim([5, 12])\n",
    "#ax[1].set_ylim([0.004, 0.008])\n",
    "\n",
    "#interpolate the two onto the same grid so we can look at the difference\n",
    "f = interp1d(lam_vec, xsub_cal, kind='linear', fill_value='extrapolate')\n",
    "ixsub_cal = f(xsub_pipe.spec[0].spec_table['WAVELENGTH'])\n",
    "\n",
    "#ax[1].plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], ixsub_cal, 'r-', label='manual, interpolated')\n",
    "\n",
    "\n",
    "diff = ((xsub_pipe.spec[0].spec_table['FLUX'] - ixsub_cal) / xsub_pipe.spec[0].spec_table['FLUX']) * 100.\n",
    "\n",
    "ax[2].plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], diff, 'k-')\n",
    "ax[2].axhline(y=1., xmin=0, xmax=1, color='r', ls='--')\n",
    "ax[2].axhline(y=-1., xmin=0, xmax=1, color='r', ls='--')\n",
    "ax[2].set_title('Difference manual - pipeline extracted spectra')\n",
    "ax[2].set_ylim([-5., 5.])\n",
    "ax[2].set_xlim([4.8, 10.2])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the ratio between the 2 is on average <= 1 per cent in the core range between 5 and 10 micron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = (xsub_pipe.spec[0].spec_table['WAVELENGTH'] >= 5.0) & (xsub_pipe.spec[0].spec_table['WAVELENGTH'] <= 10.)\n",
    "print('Mean difference between pipeline and manual extraction = {:.4f} per cent'.format(np.mean(diff[inds])))\n",
    "try:\n",
    "    assert np.mean(diff[inds]) <= 1.0, \"Mean difference between pipeline and manual extraction >= 1 per cent in 5-10 um. CHECK.\"\n",
    "except AssertionError as e:\n",
    "    print(\"****************************************************\")\n",
    "    print(\"\")\n",
    "    print(\"ERROR: {}\".format(e))\n",
    "    print(\"\")\n",
    "    print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------**END OF TEST PART 1**----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Nodded observation, two exposures\n",
    "\n",
    "In this second test we use both nodded observations. In this scenarion, the nods are used as each other's background observations and we need to ensure that the extraction aperture is placed in the right position with a realistic aperture for both nods.\n",
    "\n",
    "We will re-run the first steps of the Spec2Pipeline, so that the nods are used as each other's backgrounds. This requires creation of an association from which the Spec2Pipeline will be run. Then we will run them both through the extract_1d() step with the default parameters, checking:\n",
    "* the location of the aperture\n",
    "* the extraction width\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_files = [det1[0].meta.filename, det1[1].meta.filename]\n",
    "asn = asn_from_list(asn_files, rule=DMSLevel2bBase, meta={'program':'test', 'target':'bd60', 'asn_pool':'test'})\n",
    "\n",
    "# now add the opposite nod as background exposure:\n",
    "asn['products'][0]['members'].append({'expname': 'miri_lrs_slit_pt_nod2_v2.3_rate.fits', 'exptype':'background'})\n",
    "asn['products'][1]['members'].append({'expname': 'miri_lrs_slit_pt_nod1_v2.3_rate.fits', 'exptype':'background'})\n",
    "\n",
    "# write this out to a json file\n",
    "with open('lrs-slit-test_asn.json', 'w') as fp:\n",
    "    fp.write(asn.dump()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the Spec2Pipeline with this association files as input, instead of the individual FITS files or datamodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = Spec2Pipeline.call('lrs-slit-test_asn.json', save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1dfiles = glob('*_x1d.fits')\n",
    "print(x1dfiles)\n",
    "x1ds = [datamodels.open(xf) for xf in x1dfiles]\n",
    "fig = plt.figure(figsize=[15,5])\n",
    "\n",
    "for x1 in x1ds:\n",
    "    if 'nod1' in x1.meta.filename:\n",
    "        nn = 'nod1'\n",
    "    else: \n",
    "        nn = 'nod2'\n",
    "    plt.plot(x1.spec[0].spec_table['WAVELENGTH'], x1.spec[0].spec_table['FLUX'], label=nn)\n",
    "\n",
    "plt.plot(xsub_pipe.spec[0].spec_table['WAVELENGTH'], xsub_pipe.spec[0].spec_table['FLUX'], label='nod 1, full bbox extracted')\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()\n",
    "plt.xlim([4.5, 12.5])\n",
    "plt.xlabel('wavelength (micron)', fontsize='large')\n",
    "plt.ylabel('Jy', fontsize='large')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will test for:\n",
    "* the extracted spectra should be near-identical (chosen criteria: mean ration between the 2 <= 5%)\n",
    "\n",
    "\n",
    "**Further tests to add:**\n",
    "* perform a full verification of the extraction at the source position and with the same extraction width as in the parameters file.\n",
    "\n",
    "**If the ``assert`` statement below passes, we consider the test a success.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = x1ds[0].spec[0].spec_table['FLUX'] > 0.00\n",
    "ratio = x1ds[0].spec[0].spec_table['FLUX'][inds] / x1ds[1].spec[0].spec_table['FLUX'][inds]\n",
    "infs = np.isinf(ratio-1.0)\n",
    "\n",
    "try:\n",
    "    assert np.mean(np.abs(ratio[~infs] - 1.)) <= 0.05, \"Extracted spectra don't match! CHECK!\"\n",
    "except AssertionError as e:\n",
    "    print(\"****************************************************\")\n",
    "    print(\"\")\n",
    "    print(\"ERROR: {}\".format(e))\n",
    "    print(\"\")\n",
    "    print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Authors: B. Sargent/S. Kendrew, MIRI branch\n",
    "* Last updated: March 24th 2021 (S. Kendrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
